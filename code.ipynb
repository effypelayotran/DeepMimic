{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "YvyGCsgSCxHQ"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YvyGCsgSCxHQ"
      },
      "source": [
        "# Installation Commands (You will need to these 6 cells before running anything else in this file!)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xqo7pyX-n72M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de3bdfec-4a69-4bbd-d916-33b861059d8a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: mujoco in /usr/local/lib/python3.11/dist-packages (3.3.2)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from mujoco) (1.4.0)\n",
            "Requirement already satisfied: etils[epath] in /usr/local/lib/python3.11/dist-packages (from mujoco) (1.12.2)\n",
            "Requirement already satisfied: glfw in /usr/local/lib/python3.11/dist-packages (from mujoco) (2.9.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from mujoco) (2.0.2)\n",
            "Requirement already satisfied: pyopengl in /usr/local/lib/python3.11/dist-packages (from mujoco) (3.1.9)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from etils[epath]->mujoco) (2025.3.2)\n",
            "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.11/dist-packages (from etils[epath]->mujoco) (6.5.2)\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.11/dist-packages (from etils[epath]->mujoco) (4.13.2)\n",
            "Requirement already satisfied: zipp in /usr/local/lib/python3.11/dist-packages (from etils[epath]->mujoco) (3.21.0)\n",
            "Requirement already satisfied: mujoco_mjx in /usr/local/lib/python3.11/dist-packages (3.3.2)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from mujoco_mjx) (1.4.0)\n",
            "Requirement already satisfied: etils[epath] in /usr/local/lib/python3.11/dist-packages (from mujoco_mjx) (1.12.2)\n",
            "Requirement already satisfied: jax in /usr/local/lib/python3.11/dist-packages (from mujoco_mjx) (0.5.2)\n",
            "Requirement already satisfied: jaxlib in /usr/local/lib/python3.11/dist-packages (from mujoco_mjx) (0.5.1)\n",
            "Requirement already satisfied: mujoco>=3.3.2.dev0 in /usr/local/lib/python3.11/dist-packages (from mujoco_mjx) (3.3.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from mujoco_mjx) (1.15.2)\n",
            "Requirement already satisfied: trimesh in /usr/local/lib/python3.11/dist-packages (from mujoco_mjx) (4.6.8)\n",
            "Requirement already satisfied: glfw in /usr/local/lib/python3.11/dist-packages (from mujoco>=3.3.2.dev0->mujoco_mjx) (2.9.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from mujoco>=3.3.2.dev0->mujoco_mjx) (2.0.2)\n",
            "Requirement already satisfied: pyopengl in /usr/local/lib/python3.11/dist-packages (from mujoco>=3.3.2.dev0->mujoco_mjx) (3.1.9)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from etils[epath]->mujoco_mjx) (2025.3.2)\n",
            "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.11/dist-packages (from etils[epath]->mujoco_mjx) (6.5.2)\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.11/dist-packages (from etils[epath]->mujoco_mjx) (4.13.2)\n",
            "Requirement already satisfied: zipp in /usr/local/lib/python3.11/dist-packages (from etils[epath]->mujoco_mjx) (3.21.0)\n",
            "Requirement already satisfied: ml_dtypes>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from jax->mujoco_mjx) (0.4.1)\n",
            "Requirement already satisfied: opt_einsum in /usr/local/lib/python3.11/dist-packages (from jax->mujoco_mjx) (3.4.0)\n",
            "Requirement already satisfied: brax in /usr/local/lib/python3.11/dist-packages (0.12.3)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from brax) (1.4.0)\n",
            "Requirement already satisfied: dm-env in /usr/local/lib/python3.11/dist-packages (from brax) (1.6)\n",
            "Requirement already satisfied: etils in /usr/local/lib/python3.11/dist-packages (from brax) (1.12.2)\n",
            "Requirement already satisfied: flask in /usr/local/lib/python3.11/dist-packages (from brax) (3.1.0)\n",
            "Requirement already satisfied: flask-cors in /usr/local/lib/python3.11/dist-packages (from brax) (5.0.1)\n",
            "Requirement already satisfied: flax in /usr/local/lib/python3.11/dist-packages (from brax) (0.10.6)\n",
            "Requirement already satisfied: grpcio in /usr/local/lib/python3.11/dist-packages (from brax) (1.71.0)\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.11/dist-packages (from brax) (0.25.2)\n",
            "Requirement already satisfied: jax>=0.4.6 in /usr/local/lib/python3.11/dist-packages (from brax) (0.5.2)\n",
            "Requirement already satisfied: jaxlib>=0.4.6 in /usr/local/lib/python3.11/dist-packages (from brax) (0.5.1)\n",
            "Requirement already satisfied: jaxopt in /usr/local/lib/python3.11/dist-packages (from brax) (0.8.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from brax) (3.1.6)\n",
            "Requirement already satisfied: ml-collections in /usr/local/lib/python3.11/dist-packages (from brax) (1.1.0)\n",
            "Requirement already satisfied: mujoco in /usr/local/lib/python3.11/dist-packages (from brax) (3.3.2)\n",
            "Requirement already satisfied: mujoco-mjx in /usr/local/lib/python3.11/dist-packages (from brax) (3.3.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from brax) (2.0.2)\n",
            "Requirement already satisfied: optax in /usr/local/lib/python3.11/dist-packages (from brax) (0.2.4)\n",
            "Requirement already satisfied: orbax-checkpoint in /usr/local/lib/python3.11/dist-packages (from brax) (0.11.13)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from brax) (11.2.1)\n",
            "Requirement already satisfied: pytinyrenderer in /usr/local/lib/python3.11/dist-packages (from brax) (0.0.14)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from brax) (1.15.2)\n",
            "Requirement already satisfied: tensorboardx in /usr/local/lib/python3.11/dist-packages (from brax) (2.6.2.2)\n",
            "Requirement already satisfied: trimesh in /usr/local/lib/python3.11/dist-packages (from brax) (4.6.8)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from brax) (4.13.2)\n",
            "Requirement already satisfied: ml_dtypes>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from jax>=0.4.6->brax) (0.4.1)\n",
            "Requirement already satisfied: opt_einsum in /usr/local/lib/python3.11/dist-packages (from jax>=0.4.6->brax) (3.4.0)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.11/dist-packages (from dm-env->brax) (0.1.9)\n",
            "Requirement already satisfied: Werkzeug>=3.1 in /usr/local/lib/python3.11/dist-packages (from flask->brax) (3.1.3)\n",
            "Requirement already satisfied: itsdangerous>=2.2 in /usr/local/lib/python3.11/dist-packages (from flask->brax) (2.2.0)\n",
            "Requirement already satisfied: click>=8.1.3 in /usr/local/lib/python3.11/dist-packages (from flask->brax) (8.1.8)\n",
            "Requirement already satisfied: blinker>=1.9 in /usr/local/lib/python3.11/dist-packages (from flask->brax) (1.9.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->brax) (3.0.2)\n",
            "Requirement already satisfied: msgpack in /usr/local/lib/python3.11/dist-packages (from flax->brax) (1.1.0)\n",
            "Requirement already satisfied: tensorstore in /usr/local/lib/python3.11/dist-packages (from flax->brax) (0.1.74)\n",
            "Requirement already satisfied: rich>=11.1 in /usr/local/lib/python3.11/dist-packages (from flax->brax) (13.9.4)\n",
            "Requirement already satisfied: PyYAML>=5.4.1 in /usr/local/lib/python3.11/dist-packages (from flax->brax) (6.0.2)\n",
            "Requirement already satisfied: treescope>=0.1.7 in /usr/local/lib/python3.11/dist-packages (from flax->brax) (0.1.9)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gym->brax) (3.1.1)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.11/dist-packages (from gym->brax) (0.0.8)\n",
            "Requirement already satisfied: glfw in /usr/local/lib/python3.11/dist-packages (from mujoco->brax) (2.9.0)\n",
            "Requirement already satisfied: pyopengl in /usr/local/lib/python3.11/dist-packages (from mujoco->brax) (3.1.9)\n",
            "Requirement already satisfied: chex>=0.1.87 in /usr/local/lib/python3.11/dist-packages (from optax->brax) (0.1.89)\n",
            "Requirement already satisfied: nest_asyncio in /usr/local/lib/python3.11/dist-packages (from orbax-checkpoint->brax) (1.6.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from orbax-checkpoint->brax) (5.29.4)\n",
            "Requirement already satisfied: humanize in /usr/local/lib/python3.11/dist-packages (from orbax-checkpoint->brax) (4.12.3)\n",
            "Requirement already satisfied: simplejson>=3.16.0 in /usr/local/lib/python3.11/dist-packages (from orbax-checkpoint->brax) (3.20.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorboardx->brax) (24.2)\n",
            "Requirement already satisfied: toolz>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from chex>=0.1.87->optax->brax) (0.12.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=11.1->flax->brax) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=11.1->flax->brax) (2.19.1)\n",
            "Requirement already satisfied: attrs>=18.2.0 in /usr/local/lib/python3.11/dist-packages (from dm-tree->dm-env->brax) (25.3.0)\n",
            "Requirement already satisfied: wrapt>=1.11.2 in /usr/local/lib/python3.11/dist-packages (from dm-tree->dm-env->brax) (1.17.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from etils[epath,epy]->orbax-checkpoint->brax) (2025.3.2)\n",
            "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.11/dist-packages (from etils[epath,epy]->orbax-checkpoint->brax) (6.5.2)\n",
            "Requirement already satisfied: zipp in /usr/local/lib/python3.11/dist-packages (from etils[epath,epy]->orbax-checkpoint->brax) (3.21.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1->flax->brax) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install mujoco\n",
        "!pip install mujoco_mjx\n",
        "!pip install brax\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "IbZxYDxzoz5R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8caf9363-048b-43b5-8cff-e121568dc3ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting environment variable to use GPU rendering:\n",
            "env: MUJOCO_GL=egl\n",
            "Checking that the installation succeeded:\n",
            "Installation successful.\n"
          ]
        }
      ],
      "source": [
        "#@title Check if MuJoCo installation was successful\n",
        "\n",
        "from google.colab import files\n",
        "\n",
        "import distutils.util\n",
        "import os\n",
        "import subprocess\n",
        "if subprocess.run('nvidia-smi').returncode:\n",
        "  raise RuntimeError(\n",
        "      'Cannot communicate with GPU. '\n",
        "      'Make sure you are using a GPU Colab runtime. '\n",
        "      'Go to the Runtime menu and select Choose runtime type.')\n",
        "\n",
        "# Add an ICD config so that glvnd can pick up the Nvidia EGL driver.\n",
        "# This is usually installed as part of an Nvidia driver package, but the Colab\n",
        "# kernel doesn't install its driver via APT, and as a result the ICD is missing.\n",
        "# (https://github.com/NVIDIA/libglvnd/blob/master/src/EGL/icd_enumeration.md)\n",
        "NVIDIA_ICD_CONFIG_PATH = '/usr/share/glvnd/egl_vendor.d/10_nvidia.json'\n",
        "if not os.path.exists(NVIDIA_ICD_CONFIG_PATH):\n",
        "  with open(NVIDIA_ICD_CONFIG_PATH, 'w') as f:\n",
        "    f.write(\"\"\"{\n",
        "    \"file_format_version\" : \"1.0.0\",\n",
        "    \"ICD\" : {\n",
        "        \"library_path\" : \"libEGL_nvidia.so.0\"\n",
        "    }\n",
        "}\n",
        "\"\"\")\n",
        "\n",
        "# Configure MuJoCo to use the EGL rendering backend (requires GPU)\n",
        "print('Setting environment variable to use GPU rendering:')\n",
        "%env MUJOCO_GL=egl\n",
        "\n",
        "try:\n",
        "  print('Checking that the installation succeeded:')\n",
        "  import mujoco\n",
        "  mujoco.MjModel.from_xml_string('<mujoco/>')\n",
        "except Exception as e:\n",
        "  raise e from RuntimeError(\n",
        "      'Something went wrong during installation. Check the shell output above '\n",
        "      'for more information.\\n'\n",
        "      'If using a hosted Colab runtime, make sure you enable GPU acceleration '\n",
        "      'by going to the Runtime menu and selecting \"Choose runtime type\".')\n",
        "\n",
        "print('Installation successful.')\n",
        "\n",
        "# Tell XLA to use Triton GEMM, this improves steps/sec by ~30% on some GPUs\n",
        "xla_flags = os.environ.get('XLA_FLAGS', '')\n",
        "xla_flags += ' --xla_gpu_triton_gemm_any=True'\n",
        "os.environ['XLA_FLAGS'] = xla_flags\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T5f4w3Kq2X14",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6343a375-db75-4fe3-8fd7-8f039120e79e",
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing mediapy:\n"
          ]
        }
      ],
      "source": [
        "#@title Import packages for plotting and creating graphics\n",
        "import time\n",
        "import itertools\n",
        "import numpy as np\n",
        "from typing import Callable, NamedTuple, Optional, Union, List\n",
        "\n",
        "# Graphics and plotting.\n",
        "print('Installing mediapy:')\n",
        "!command -v ffmpeg >/dev/null || (apt update && apt install -y ffmpeg)\n",
        "!pip install -q mediapy\n",
        "import mediapy as media\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# More legible printing from numpy.\n",
        "np.set_printoptions(precision=3, suppress=True, linewidth=100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ObF1UXrkb0Nd",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Import MuJoCo, MJX, and Brax\n",
        "from datetime import datetime\n",
        "from etils import epath\n",
        "import functools\n",
        "from IPython.display import HTML\n",
        "from typing import Any, Dict, Sequence, Tuple, Union\n",
        "import os\n",
        "from ml_collections import config_dict\n",
        "\n",
        "\n",
        "import jax\n",
        "from jax import numpy as jp # jp is jax numpy\n",
        "import numpy as np\n",
        "from flax.training import orbax_utils\n",
        "from flax import struct\n",
        "from matplotlib import pyplot as plt\n",
        "import mediapy as media\n",
        "from orbax import checkpoint as ocp\n",
        "\n",
        "import mujoco\n",
        "from mujoco import mjx\n",
        "\n",
        "from brax import base\n",
        "from brax import envs\n",
        "from brax import math\n",
        "from brax.base import Base, Motion, Transform\n",
        "from brax.base import State as PipelineState\n",
        "from brax.envs.base import Env, PipelineEnv, State\n",
        "from brax.mjx.base import State as MjxState\n",
        "from brax.training.agents.ppo import train as ppo\n",
        "from brax.training.agents.ppo import networks as ppo_networks\n",
        "from brax.io import html, mjcf, model\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Mount GDrive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0BMGpzf8Ck8e",
        "outputId": "36c78529-c014-4fbe-857f-a5092c654fdc",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/effypelayotran/mujoco_resources.git"
      ],
      "metadata": {
        "id": "fK2I4fjOFCzH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "009209a3-1676-4ea6-a1e8-91771e15730f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'mujoco_resources' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define Humanoid Env and Reward"
      ],
      "metadata": {
        "id": "tf4lpveF_0jb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Parse the Motion Capture into qpos & qvels\n",
        "\n",
        "from pathlib import Path\n",
        "import os\n",
        "from scipy.spatial.transform import Rotation as R\n",
        "import numpy as np\n",
        "\n",
        "tasks = {}\n",
        "\n",
        "\n",
        "def parse_bvh_file():\n",
        "\tprint(\"Parsing\")\n",
        "\t# Path to current script\n",
        "\t#current_file = Path(__file__)\n",
        "\n",
        "\t# Go two levels up to reach 'parent' folder\n",
        "\t#parent_dir = current_file.parent.parent\n",
        "\n",
        "\tmocap_folder = Path('/content/drive/MyDrive/make_him_walk/mocap')\n",
        "\t#mocap_folder = drive_folder / 'mocap'\n",
        "\n",
        "\tprint(\"got mocap folder\")\n",
        "\n",
        "\t#go through all the different types of actions in the mocap folder\n",
        "\tfor action in mocap_folder.iterdir():\n",
        "\t\tif action.is_dir():\n",
        "\t\t\t#for each action make a string value in the tasks dictionary so we can map a list of clips to it\n",
        "\t\t\ttasks[action.name] = []\n",
        "\t\t\tcurr_action = tasks[action.name]\n",
        "\t\t\t#get each individual clip\n",
        "\t\t\tfor clip in action.glob('*.bvh'):  # Only files in this subfolder, not recursive\n",
        "\t\t\t\t#for each clip we append to the list of clips for the given action\n",
        "\t\t\t\tcurr_action.append([])\n",
        "\t\t\t\t#access last element in the list of clips corresponding to the action we're looking at (this is the clip we are looking at)\n",
        "\t\t\t\tcurr_clip = curr_action[-1]\n",
        "\t\t\t\tframe_data = []\n",
        "\t\t\t\twith open(clip, 'r', encoding='utf-8') as f:\n",
        "\t\t\t\t\tframe_num = 0\n",
        "\t\t\t\t\tjoints = []\n",
        "\t\t\t\t\tmotion_data_started = False\n",
        "\t\t\t\t\tcurrent_frame = []\n",
        "\n",
        "\t\t\t\t\t#now we want to read through the clip and append a dict of all the joint info every frame to the list we just created representing that clip\n",
        "\t\t\t\t\tfor line in f:\n",
        "\t\t\t\t\t\tstripped = line.strip()\n",
        "\t\t\t\t\t\tif not stripped:\n",
        "\t\t\t\t\t\t\tcontinue  # skip blank lines\n",
        "\n",
        "\t\t\t\t\t\tif stripped.startswith('ROOT') or stripped.startswith('JOINT'):\n",
        "\t\t\t\t\t\t\tjoint_name = stripped.split()[1]\n",
        "\t\t\t\t\t\t\tprint(\"found JOINT: \", joint_name)\n",
        "\t\t\t\t\t\t\tjoints.append(joint_name)\n",
        "\t\t\t\t\t\t\tcontinue\n",
        "\n",
        "\t\t\t\t\t\tif stripped.startswith('Frame Time:'):\n",
        "\t\t\t\t\t\t\tmotion_data_started = True\n",
        "\t\t\t\t\t\t\tprint(\"Looking at motion data\")\n",
        "\t\t\t\t\t\t\tcontinue\n",
        "\n",
        "\t\t\t\t\t\t#at this point we can start storing the joint data for each frame\n",
        "\t\t\t\t\t\tif motion_data_started:\n",
        "\t\t\t\t\t\t\t#store all the floats representing the joint data in current_frame\n",
        "\t\t\t\t\t\t\tfloats = list(map(float, stripped.split()))\n",
        "\t\t\t\t\t\t\tcurrent_frame.extend(floats)\n",
        "\n",
        "\t\t\t\t\t\t\t# Once we have 78 floats, store the frame and reset the list\n",
        "\t\t\t\t\t\t\tif len(current_frame) >= 78:\n",
        "\t\t\t\t\t\t\t\tframe_num += 1\n",
        "\t\t\t\t\t\t\t\tcurr_clip.append({})\n",
        "\t\t\t\t\t\t\t\tframe = curr_clip[-1]\n",
        "\t\t\t\t\t\t\t\tframe_data.append(current_frame[:78])  # Store the first 78 floats\n",
        "\t\t\t\t\t\t\t\tcurr_frame_data = frame_data[-1]\n",
        "\t\t\t\t\t\t\t\tcurrent_frame = current_frame[78:]  # Keep the remaining floats for next frame\n",
        "\t\t\t\t\t\t\t\tfor i in range (len(joints)):\n",
        "\t\t\t\t\t\t\t\t\tjoint_name = joints[i]\n",
        "\t\t\t\t\t\t\t\t\tframe[joint_name] = {}\n",
        "\t\t\t\t\t\t\t\t\t#if we are looking at the hips\n",
        "\t\t\t\t\t\t\t\t\tif i == 0:\n",
        "\t\t\t\t\t\t\t\t\t\thips = frame[joint_name]\n",
        "\t\t\t\t\t\t\t\t\t\t#hard code position and rotation for the hips since it's the only one with those two attirbutes\n",
        "\t\t\t\t\t\t\t\t\t\thips[\"position\"] = np.array(curr_frame_data[:3])\n",
        "\n",
        "\t\t\t\t\t\t\t\t\t\t#THIS IS WHERE WE SWITCH AXES !!!!!!!!\n",
        "\t\t\t\t\t\t\t\t\t\t#!!!!!!!!!!!!\n",
        "\t\t\t\t\t\t\t\t\t\t#\n",
        "\n",
        "\t\t\t\t\t\t\t\t\t\thips[\"position\"][0], hips[\"position\"][1], hips[\"position\"][2] = hips[\"position\"][2], hips[\"position\"][0], hips[\"position\"][1]\n",
        "\n",
        "                    # NEW\n",
        "\t\t\t\t\t\t\t\t\t\tz_rot, x_rot, y_rot = curr_frame_data[3:6]\n",
        "\t\t\t\t\t\t\t\t\t\trot_bvh = R.from_euler('YZX', [z_rot, x_rot, y_rot], degrees=True)\n",
        "\n",
        "\t\t\t\t\t\t\t\t\t\tbvh_to_mj = R.from_euler('X', 0, degrees=True)\n",
        "\t\t\t\t\t\t\t\t\t\tturn_around_bitch = R.from_euler('Z', 180, degrees=True)\n",
        "\t\t\t\t\t\t\t\t\t\tquat = rot_bvh #bvh_to_mj * rot_bvh\n",
        "\t\t\t\t\t\t\t\t\t\thips[\"rotation\"] = quat\n",
        "\n",
        "\t\t\t\t\t\t\t\t\t\t# rotation = curr_frame_data[3:6]\n",
        "\n",
        "\t\t\t\t\t\t\t\t\t\t# #SWITCH AXES TO BE CONSITENT WITH MUJOCO\n",
        "\t\t\t\t\t\t\t\t\t\t# rotation[0], rotation[1], rotation[2] = (-1 * rotation[2]), rotation[0], rotation[1]\n",
        "\n",
        "\t\t\t\t\t\t\t\t\t\t# rot_bvh = R.from_euler('XYZ', rotation, degrees=True)\n",
        "\n",
        "\t\t\t\t\t\t\t\t\t\t# # x_adj = R.from_euler('x', -90, degrees=True)\n",
        "\t\t\t\t\t\t\t\t\t\t# # z_adj = R.from_euler('z', -90, degrees=True)\n",
        "\n",
        "\t\t\t\t\t\t\t\t\t\t# # transform = z_adj * x_adj\n",
        "\t\t\t\t\t\t\t\t\t\t# # transform = x_adj\n",
        "\n",
        "\t\t\t\t\t\t\t\t\t\t# # rot_bvh = transform * rot_bvh\n",
        "\t\t\t\t\t\t\t\t\t\t# quat = rot_bvh.as_quat()\n",
        "\t\t\t\t\t\t\t\t\t\t# hips[\"rotation\"] = quat\n",
        "\n",
        "\t\t\t\t\t\t\t\t\t\t# #THINK WE NEED QUATERNION FOR THE HIPS\n",
        "\t\t\t\t\t\t\t\t\t\t# quaternion = R.from_euler('xyz', rotation, degrees=True)\n",
        "\t\t\t\t\t\t\t\t\t\t# hips[\"rotation\"] = quaternion\n",
        "\n",
        "\t\t\t\t\t\t\t\t\t\t#I THINK WE WANT QUATERNION FOR THE HIPS SO I ACTUALLY COMMENT OUT THE ROTATION LINE\n",
        "\t\t\t\t\t\t\t\t\t\t#hips[\"rotation\"] = rotation\n",
        "\t\t\t\t\t\t\t\t\t\t#any other joint\n",
        "\t\t\t\t\t\t\t\t\telse:\n",
        "\t\t\t\t\t\t\t\t\t\tjoint = frame[joint_name]\n",
        "\t\t\t\t\t\t\t\t\t\tstart_ind = (i+1) * 3\n",
        "\t\t\t\t\t\t\t\t\t\tend_ind = (i+2) * 3\n",
        "\t\t\t\t\t\t\t\t\t\trotation = curr_frame_data[start_ind:end_ind]\n",
        "\t\t\t\t\t\t\t\t\t\t#SWITCH AXES TO BE CONSITENT WITH MUJOCO\n",
        "\t\t\t\t\t\t\t\t\t\trotation[0], rotation[1], rotation[2] = rotation[2], rotation[0], rotation[1]\n",
        "\n",
        "\t\t\t\t\t\t\t\t\t\t#COMMENTING OUT QUATERNION STUFF SINCE WE DONT WANT TO USE IT\n",
        "\t\t\t\t\t\t\t\t\t\t#quaternion = R.from_euler('xyz', rotation, degrees=True)\n",
        "\n",
        "\t\t\t\t\t\t\t\t\t\t#CONVERT TO RADIANS SINCE IT STARTS OUT IN DEGREES\n",
        "\t\t\t\t\t\t\t\t\t\tjoint[\"rotation\"] = np.deg2rad(rotation)\n",
        "\n",
        "\t\t\t\t#go through frames to calculate velocity\n",
        "\t\t\t\tfor f in range (len(curr_clip)):\n",
        "\t\t\t\t\tframe = curr_clip[f]\n",
        "\n",
        "\t\t\t\t\t#if we're at the end of the list we don't want to check rotation in next frame since there is no next frame\n",
        "\t\t\t\t\tif f == (len(curr_clip) - 1):\n",
        "\t\t\t\t\t\tprev_frame = curr_clip[f-1]\n",
        "\t\t\t\t\t\tfor joint in frame.keys():\n",
        "\t\t\t\t\t\t\t#just set the final velocity to whatever the previous frame's velocity was\n",
        "\t\t\t\t\t\t\tframe[joint][\"velocity\"] = prev_frame[joint][\"velocity\"]\n",
        "\n",
        "\t\t\t\t\telse:\n",
        "\t\t\t\t\t\tnext_frame = curr_clip[f + 1]\n",
        "\t\t\t\t\t\tfor joint in frame.keys():\n",
        "\t\t\t\t\t\t\tframe1_rot = frame[joint][\"rotation\"]\n",
        "\t\t\t\t\t\t\tframe2_rot = next_frame[joint][\"rotation\"]\n",
        "\n",
        "\t\t\t\t\t\t\t#HIPS ARE QUATERNION, OTHER JOINTS ARE RADIANS\n",
        "\t\t\t\t\t\t\tif joint == \"Hips\":\n",
        "\t\t\t\t\t\t\t\trot_diff = frame2_rot * frame1_rot.inv()\n",
        "\t\t\t\t\t\t\t\tdiff_angle = rot_diff.as_rotvec()\n",
        "\t\t\t\t\t\t\t\tframe[joint][\"velocity\"] = diff_angle\n",
        "\n",
        "\t\t\t\t\t\t\telse:\n",
        "\t\t\t\t\t\t\t\tdiff_angle = frame2_rot - frame1_rot\n",
        "\t\t\t\t\t\t\t#DIVIDE BY DT WHICH WILL BE PASSED THROUGH AND DETERMINED BY THE MUJOCO ENVIRONMENT\n",
        "\t\t\t\t\t\t\tvel = diff_angle\n",
        "\t\t\t\t\t\t\tframe[joint][\"velocity\"] = vel\n",
        "\n",
        "\t\t\t\t\t#FOR NOW NOT USING END EFFECTS (MIGHT DELETE COMPLETELY TBH)\n",
        "\t\t\t\t\t#NEXT FIND POSITIONS OF HANDS AND FEET\n",
        "\t\t\t\t\t#find hip rotation and position first to start accumalation\n",
        "\t\t\t\t\t# hip_world_pos = frame[\"Hips\"][\"position\"]\n",
        "\t\t\t\t\t# hip_world_rot = frame[\"Hips\"][\"rotation\"]\n",
        "\n",
        "\t\t\t\t\t# #spine\n",
        "\t\t\t\t\t# spine_offset = np.array([0.00000, 0.00000, 0.03937])\n",
        "\t\t\t\t\t# #apply parent rotation (being the hip rot) to get the actual position in world space\n",
        "\t\t\t\t\t# spine_world = hip_world_rot.apply(spine_offset)\n",
        "\t\t\t\t\t# #now create world_pos as a new variable so that we can save the hip rot and pos\n",
        "\t\t\t\t\t# world_pos = hip_world_pos + spine_world\n",
        "\t\t\t\t\t# world_rot = hip_world_rot * frame[\"Spine\"][\"rotation\"]\n",
        "\n",
        "\t\t\t\t\t# #spine1\n",
        "\t\t\t\t\t# spine1_offset = np.array([0.00000, 0.00000, 10.24829])\n",
        "\t\t\t\t\t# spine1_world = world_rot.apply(spine1_offset)\n",
        "\t\t\t\t\t# world_pos = world_pos + spine1_world\n",
        "\t\t\t\t\t# world_rot = world_rot * frame[\"Spine1\"][\"rotation\"]\n",
        "\n",
        "\t\t\t\t\t# #right shoulder\n",
        "\t\t\t\t\t# r_should_offset = np.array([0.00000, 0.00000, 7.82687])\n",
        "\t\t\t\t\t# r_should_world = world_rot.apply(r_should_offset)\n",
        "\t\t\t\t\t# #change to right world pos/rotation now since we'll want to save pos/rot of spine for the left side\n",
        "\t\t\t\t\t# right_world_pos = world_pos + r_should_world\n",
        "\t\t\t\t\t# right_world_rot = world_rot * frame[\"RightShoulder\"][\"rotation\"]\n",
        "\n",
        "\t\t\t\t\t# #right arm\n",
        "\t\t\t\t\t# r_arm_offset = np.array([0.00000, -6.71018, -0.00002])\n",
        "\t\t\t\t\t# r_arm_world = right_world_rot.apply(r_arm_offset)\n",
        "\t\t\t\t\t# right_world_pos = right_world_pos + r_arm_world\n",
        "\t\t\t\t\t# right_world_rot = right_world_rot * frame[\"RightArm\"][\"rotation\"]\n",
        "\n",
        "\t\t\t\t\t# #right forearm\n",
        "\t\t\t\t\t# r_forearm_offset = np.array([0.00000, -10.94419, -0.00004])\n",
        "\t\t\t\t\t# r_forearm_world = right_world_rot.apply(r_forearm_offset)\n",
        "\t\t\t\t\t# right_world_pos = right_world_pos + r_forearm_world\n",
        "\t\t\t\t\t# right_world_rot = right_world_rot * frame[\"RightForeArm\"][\"rotation\"]\n",
        "\n",
        "\t\t\t\t\t# #right hand\n",
        "\t\t\t\t\t# r_hand_offset = np.array([0.00000 ,-8.52010, -0.00003])\n",
        "\t\t\t\t\t# r_hand_world = right_world_rot.apply(r_hand_offset)\n",
        "\t\t\t\t\t# r_hand_world = right_world_pos + r_hand_world\n",
        "\n",
        "\t\t\t\t\t# frame[\"RightHand\"][\"position\"] = r_hand_world\n",
        "\n",
        "\t\t\t\t\t# #left shoulder\n",
        "\t\t\t\t\t# l_should_offset = np.array([0.00000, 0.00000, 7.82687])\n",
        "\t\t\t\t\t# l_should_world = world_rot.apply(l_should_offset)\n",
        "\t\t\t\t\t# #now do l world pos/rot\n",
        "\t\t\t\t\t# l_world_pos = world_pos + l_should_world\n",
        "\t\t\t\t\t# l_world_rot = world_rot * frame[\"LeftShoulder\"][\"rotation\"]\n",
        "\n",
        "\t\t\t\t\t# #left arm\n",
        "\t\t\t\t\t# l_arm_offset = np.array([0.00000, 6.71018, -0.00002])\n",
        "\t\t\t\t\t# l_arm_world = l_world_rot.apply(l_arm_offset)\n",
        "\t\t\t\t\t# l_world_pos = l_world_pos + l_arm_world\n",
        "\t\t\t\t\t# l_world_rot = l_world_rot * frame[\"LeftArm\"][\"rotation\"]\n",
        "\n",
        "\t\t\t\t\t# #left forearm\n",
        "\t\t\t\t\t# l_forearm_offset = np.array([0.00000, 10.94419, -0.00004])\n",
        "\t\t\t\t\t# l_forearm_world = l_world_rot.apply(l_forearm_offset)\n",
        "\t\t\t\t\t# l_world_pos = l_world_pos + l_forearm_world\n",
        "\t\t\t\t\t# l_world_rot = l_world_rot * frame[\"LeftForeArm\"][\"rotation\"]\n",
        "\n",
        "\t\t\t\t\t# #left hand\n",
        "\t\t\t\t\t# l_hand_offset = np.array([0.00000 ,8.52010, -0.00003])\n",
        "\t\t\t\t\t# l_hand_world = l_world_rot.apply(l_hand_offset)\n",
        "\t\t\t\t\t# l_hand_world = l_world_pos + l_hand_world\n",
        "\n",
        "\t\t\t\t\t# frame[\"LeftHand\"][\"position\"] = l_hand_world\n",
        "\n",
        "\t\t\t\t\t# #left upper leg\n",
        "\t\t\t\t\t# #we start back at the hips and luckily we stored it earlier and didnt change it during accumalation\n",
        "\t\t\t\t\t# l_up_leg_offset = np.array([0.00000, 3.64953, 0.00000])\n",
        "\t\t\t\t\t# l_up_leg_world = hip_world_rot.apply(l_up_leg_offset)\n",
        "\t\t\t\t\t# #now do l world pos/rot again since we're not doing anything else with the left hand\n",
        "\t\t\t\t\t# l_world_pos = hip_world_pos + l_up_leg_world\n",
        "\t\t\t\t\t# l_world_rot = hip_world_rot * frame[\"LeftUpLeg\"][\"rotation\"]\n",
        "\n",
        "\t\t\t\t\t# #left leg\n",
        "\t\t\t\t\t# l_leg_offset = np.array([0.00000, 0.00000, -15.70580])\n",
        "\t\t\t\t\t# l_leg_world = l_world_rot.apply(l_leg_offset)\n",
        "\t\t\t\t\t# l_world_pos = l_world_pos + l_leg_world\n",
        "\t\t\t\t\t# l_world_rot = l_world_rot * frame[\"LeftLeg\"][\"rotation\"]\n",
        "\n",
        "\t\t\t\t\t# #left foot\n",
        "\t\t\t\t\t# l_foot_offset = np.array([0.00000, 0.00000, -15.41867])\n",
        "\t\t\t\t\t# l_foot_world = l_world_rot.apply(l_foot_offset)\n",
        "\t\t\t\t\t# l_foot_world = l_world_pos + l_foot_world\n",
        "\n",
        "\t\t\t\t\t# frame[\"LeftFoot\"][\"position\"] = l_foot_world\n",
        "\n",
        "\t\t\t\t\t# #right upper leg\n",
        "\t\t\t\t\t# r_up_leg_offset = np.array([0.00000, -3.64953, 0.00000])\n",
        "\t\t\t\t\t# r_up_leg_world = hip_world_rot.apply(r_up_leg_offset)\n",
        "\t\t\t\t\t# #now do l world pos/rot again since we're not doing anything else with the left hand\n",
        "\t\t\t\t\t# r_world_pos = hip_world_pos + r_up_leg_world\n",
        "\t\t\t\t\t# r_world_rot = hip_world_rot * frame[\"RightUpLeg\"][\"rotation\"]\n",
        "\n",
        "\t\t\t\t\t# #right leg\n",
        "\t\t\t\t\t# r_leg_offset = np.array([0.00000, 0.00000, -15.70580])\n",
        "\t\t\t\t\t# r_leg_world = r_world_rot.apply(r_leg_offset)\n",
        "\t\t\t\t\t# r_world_pos = r_world_pos + r_leg_world\n",
        "\t\t\t\t\t# r_world_rot = r_world_rot * frame[\"RightLeg\"][\"rotation\"]\n",
        "\n",
        "\t\t\t\t\t# #right foot\n",
        "\t\t\t\t\t# r_foot_offset = np.array([0.00000, 0.00000, -15.41867])\n",
        "\t\t\t\t\t# r_foot_world = r_world_rot.apply(r_foot_offset)\n",
        "\t\t\t\t\t# r_foot_world = r_world_pos + r_foot_world\n",
        "\n",
        "\t\t\t\t\t# frame[\"RightFoot\"][\"position\"] = r_foot_world\n",
        "\n",
        "\tbvh_to_mujoco()\n",
        "\treturn tasks\n",
        "\n",
        "\n",
        "def bvh_to_mujoco():\n",
        "\tfor task in tasks.keys():\n",
        "\t\tprint(\"TASK \", task)\n",
        "\t\tfor clip in tasks[task]:\n",
        "\t\t\tfor frame in clip:\n",
        "\t\t\t\tqpos = []\n",
        "\t\t\t\tqpos.append(frame[\"Hips\"][\"position\"][0])\n",
        "\t\t\t\tqpos.append(frame[\"Hips\"][\"position\"][1])\n",
        "\t\t\t\tqpos.append(frame[\"Hips\"][\"position\"][2])\n",
        "\n",
        "\t\t\t\t#WE DO THIS WEIRD ORDER BECAUSE MUJOCO ACTUALLY USED W,X,Y,Z WHILE SCIPY USED X,Y,Z,W\n",
        "\t\t\t\tqpos.append(frame[\"Hips\"][\"rotation\"].as_quat()[3])\n",
        "\t\t\t\tqpos.append(frame[\"Hips\"][\"rotation\"].as_quat()[0])\n",
        "\t\t\t\tqpos.append(frame[\"Hips\"][\"rotation\"].as_quat()[1])\n",
        "\t\t\t\tqpos.append(frame[\"Hips\"][\"rotation\"].as_quat()[2])\n",
        "\n",
        "\t\t\t\t#waist_lower\n",
        "\t\t\t\t#z rotation\n",
        "\t\t\t\tqpos.append(frame[\"Spine\"][\"rotation\"][2])\n",
        "\t\t\t\t#y rotation\n",
        "\t\t\t\tqpos.append(frame[\"Spine\"][\"rotation\"][1])\n",
        "\n",
        "\t\t\t\t#torso y\n",
        "\t\t\t\tqpos.append(frame[\"Spine1\"][\"rotation\"][1])\n",
        "\n",
        "\t\t\t\t#upper_arm_right\n",
        "\t\t\t\tqpos.append(frame[\"RightArm\"][\"rotation\"][0])\n",
        "\t\t\t\tqpos.append(frame[\"RightArm\"][\"rotation\"][1] * -1)\n",
        "\n",
        "\t\t\t\t#elbow_right\n",
        "\t\t\t\tqpos.append(frame[\"RightForeArm\"][\"rotation\"][2])\n",
        "\n",
        "\n",
        "\t\t\t\t#upper_arm_left\n",
        "\n",
        "\t\t\t\tqpos.append(frame[\"LeftArm\"][\"rotation\"][0])\n",
        "\t\t\t\tqpos.append(frame[\"LeftArm\"][\"rotation\"][1])\n",
        "\n",
        "\t\t\t\t#elbow_left\n",
        "\t\t\t\t#0,-1,-1\n",
        "\t\t\t\tqpos.append(frame[\"LeftForeArm\"][\"rotation\"][2] * -1)\n",
        "\n",
        "\t\t\t\t#thigh_right\n",
        "\t\t\t\tqpos.append(frame[\"RightUpLeg\"][\"rotation\"][0])\n",
        "\t\t\t\tqpos.append(frame[\"RightUpLeg\"][\"rotation\"][2])\n",
        "\t\t\t\tqpos.append(frame[\"RightUpLeg\"][\"rotation\"][1])\n",
        "\n",
        "\t\t\t\t#knee_right\n",
        "\t\t\t\t#multiply by -1 because axis is 0,-1,0\n",
        "\t\t\t\tqpos.append(frame[\"RightLeg\"][\"rotation\"][1] * -1)\n",
        "\n",
        "\t\t\t\t#foot_right\n",
        "\t\t\t\tqpos.append(frame[\"RightFoot\"][\"rotation\"][1])\n",
        "\t\t\t\t#second axis is 1,0,.5\n",
        "\t\t\t\taxis = np.array([1,0,0.5])\n",
        "\t\t\t\taxis = axis/np.linalg.norm(axis)\n",
        "\t\t\t\tproj = np.dot(frame[\"RightFoot\"][\"rotation\"], axis)\n",
        "\t\t\t\tqpos.append(proj)\n",
        "\n",
        "\n",
        "\t\t\t\t#thigh_left\n",
        "\t\t\t\tqpos.append(frame[\"LeftUpLeg\"][\"rotation\"][0] * -1)\n",
        "\t\t\t\tqpos.append(frame[\"LeftUpLeg\"][\"rotation\"][2] * -1)\n",
        "\t\t\t\tqpos.append(frame[\"LeftUpLeg\"][\"rotation\"][1])\n",
        "\n",
        "\t\t\t\t#knee_left\n",
        "\t\t\t\t#multiply by -1 because axis is 0,-1,0\n",
        "\t\t\t\tqpos.append(frame[\"LeftLeg\"][\"rotation\"][1] * -1)\n",
        "\n",
        "\t\t\t\t#foot_left\n",
        "\t\t\t\tqpos.append(frame[\"LeftFoot\"][\"rotation\"][1])\n",
        "\t\t\t\t#second axis is -1,0,-.5\n",
        "\t\t\t\taxis = np.array([-1,0,-0.5])\n",
        "\t\t\t\taxis = axis/np.linalg.norm(axis)\n",
        "\t\t\t\tproj = np.dot(frame[\"LeftFoot\"][\"rotation\"], axis)\n",
        "\t\t\t\tqpos.append(proj)\n",
        "\n",
        "\t\t\t\tframe[\"qpos\"] = qpos\n",
        "\n",
        "\t\t\t\tqvel = []\n",
        "\n",
        "\t\t\t\t#FOR NOW JUST PUT THE FIRST 3 ENTRIES AS 0 SINCE WE'RE CURRENTLY NOT IMPLEMENTING HIPS POSITION\n",
        "\t\t\t\tfor i in range (3):\n",
        "\t\t\t\t\tqvel.append(0)\n",
        "\n",
        "\t\t\t\tqvel.append(frame[\"Hips\"][\"velocity\"][0])\n",
        "\t\t\t\tqvel.append(frame[\"Hips\"][\"velocity\"][1])\n",
        "\t\t\t\tqvel.append(frame[\"Hips\"][\"velocity\"][2])\n",
        "\t\t\t\t#waist_lower\n",
        "\t\t\t\t#z rotation\n",
        "\t\t\t\tqvel.append(frame[\"Spine\"][\"velocity\"][2])\n",
        "\t\t\t\t#y rotation\n",
        "\t\t\t\tqvel.append(frame[\"Spine\"][\"velocity\"][1])\n",
        "\n",
        "\t\t\t\t#torso y\n",
        "\t\t\t\tqvel.append(frame[\"Spine1\"][\"velocity\"][1])\n",
        "\n",
        "\t\t\t\t#upper_arm_right\n",
        "\t\t\t\tqvel.append(frame[\"RightArm\"][\"velocity\"][0])\n",
        "\t\t\t\tqvel.append(frame[\"RightArm\"][\"velocity\"][1] * -1)\n",
        "\n",
        "\t\t\t\t#elbow_right\n",
        "\t\t\t\tqvel.append(frame[\"RightForeArm\"][\"velocity\"][2])\n",
        "\n",
        "\n",
        "\t\t\t\t#upper_arm_left\n",
        "\n",
        "\t\t\t\tqvel.append(frame[\"LeftArm\"][\"velocity\"][0])\n",
        "\t\t\t\tqvel.append(frame[\"LeftArm\"][\"velocity\"][1])\n",
        "\n",
        "\t\t\t\t#elbow_left\n",
        "\t\t\t\t#0,-1,-1\n",
        "\t\t\t\tqvel.append(frame[\"LeftForeArm\"][\"velocity\"][2] * -1)\n",
        "\n",
        "\t\t\t\t#thigh_right\n",
        "\t\t\t\tqvel.append(frame[\"RightUpLeg\"][\"velocity\"][0])\n",
        "\t\t\t\tqvel.append(frame[\"RightUpLeg\"][\"velocity\"][2])\n",
        "\t\t\t\tqvel.append(frame[\"RightUpLeg\"][\"velocity\"][1])\n",
        "\n",
        "\t\t\t\t#knee_right\n",
        "\t\t\t\t#multiply by -1 because axis is 0,-1,0\n",
        "\t\t\t\tqvel.append(frame[\"RightLeg\"][\"velocity\"][1] * -1)\n",
        "\n",
        "\t\t\t\t#foot_right\n",
        "\t\t\t\tqvel.append(frame[\"RightFoot\"][\"velocity\"][1])\n",
        "\t\t\t\t#second axis is 1,0,.5\n",
        "\t\t\t\taxis = np.array([1,0,0.5])\n",
        "\t\t\t\taxis = axis/np.linalg.norm(axis)\n",
        "\t\t\t\tproj = np.dot(frame[\"RightFoot\"][\"velocity\"], axis)\n",
        "\t\t\t\tqvel.append(proj)\n",
        "\n",
        "\n",
        "\t\t\t\t#thigh_left\n",
        "\t\t\t\tqvel.append(frame[\"LeftUpLeg\"][\"velocity\"][0] * -1)\n",
        "\t\t\t\tqvel.append(frame[\"LeftUpLeg\"][\"velocity\"][2] * -1)\n",
        "\t\t\t\tqvel.append(frame[\"LeftUpLeg\"][\"velocity\"][1])\n",
        "\n",
        "\t\t\t\t#knee_left\n",
        "\t\t\t\t#multiply by -1 because axis is 0,-1,0\n",
        "\t\t\t\tqvel.append(frame[\"LeftLeg\"][\"velocity\"][1] * -1)\n",
        "\n",
        "\t\t\t\t#foot_left\n",
        "\t\t\t\tqvel.append(frame[\"LeftFoot\"][\"velocity\"][1])\n",
        "\t\t\t\t#second axis is -1,0,-.5\n",
        "\t\t\t\taxis = np.array([-1,0,-0.5])\n",
        "\t\t\t\taxis = axis/np.linalg.norm(axis)\n",
        "\t\t\t\tproj = np.dot(frame[\"LeftFoot\"][\"velocity\"], axis)\n",
        "\t\t\t\tqvel.append(proj)\n",
        "\n",
        "\t\t\t\tframe[\"qvel\"] = qvel\n",
        "\n",
        "\n",
        "#COMMENTING THIS OUT (WILL PROBABLY DELETE) SINCE WE'RE NOT USING QUATERNIONS ANYMORE\n",
        "# def quat_from_xyz(x_angle=0.0, y_angle=0.0, z_angle=0.0):\n",
        "# \tcx = jp.cos(x_angle / 2)\n",
        "# \tsx = jp.sin(x_angle / 2)\n",
        "# \tcy = jp.cos(y_angle / 2)\n",
        "# \tsy = jp.sin(y_angle / 2)\n",
        "# \tcz = jp.cos(z_angle / 2)\n",
        "# \tsz = jp.sin(z_angle / 2)\n",
        "\n",
        "# \tqx = jp.array([cx, sx, 0, 0])\n",
        "# \tqy = jp.array([cy, 0, sy, 0])\n",
        "# \tqz = jp.array([cz, 0, 0, sz])\n",
        "\n",
        "# \t# Multiply quaternions: q = qx * (qy * qz)\n",
        "# \tdef quat_mul(q1, q2):\n",
        "# \t\t\tw1, x1, y1, z1 = q1\n",
        "# \t\t\tw2, x2, y2, z2 = q2\n",
        "# \t\t\treturn jp.array([\n",
        "# \t\t\t\t\tw1*w2 - x1*x2 - y1*y2 - z1*z2,\n",
        "# \t\t\t\t\tw1*x2 + x1*w2 + y1*z2 - z1*y2,\n",
        "# \t\t\t\t\tw1*y2 - x1*z2 + y1*w2 + z1*x2,\n",
        "# \t\t\t\t\tw1*z2 + x1*y2 - y1*x2 + z1*w2\n",
        "# \t\t\t])\n",
        "\n",
        "# \tq = quat_mul(qx, quat_mul(qy, qz))\n",
        "# \treturn q\n",
        "\n",
        "# def mujoco_to_quaternion(qpos):\n",
        "\n",
        "# \tquaternions=jp.array([])\n",
        "\n",
        "# \t#hip data\n",
        "# \tquaternions = jp.concatenate([quaternions, jp.array(qpos[:7])])\n",
        "\n",
        "# \t#lower waist\n",
        "# \tabdomen_z, abdomen_y = qpos[7:9]\n",
        "# \tabdomen_quat = quat_from_xyz(y_angle=abdomen_y, z_angle=abdomen_z)\n",
        "# \tquaternions = jp.concatenate([quaternions, abdomen_quat])\n",
        "\n",
        "\n",
        "# \t#torso\n",
        "# \ttorso_y = qpos[9]\n",
        "# \ttorso_quat = quat_from_xyz(y_angle = torso_y)\n",
        "# \tquaternions = jp.concatenate([quaternions, torso_quat])\n",
        "\n",
        "\n",
        "# \t#upper right arm\n",
        "# \tshould1_right, should2_right = qpos[10:12]\n",
        "# \tshould_right_quat = quat_from_xyz(x_angle = should1_right, y_angle = should2_right)\n",
        "# \tquaternions = jp.concatenate([quaternions, should_right_quat])\n",
        "\n",
        "\n",
        "# \t#right elbow\n",
        "# \telbow_right = qpos[12]\n",
        "# \telbow_right_quat = quat_from_xyz(y_angle = elbow_right)\n",
        "# \tquaternions = jp.concatenate([quaternions, elbow_right_quat])\n",
        "\n",
        "\n",
        "# \t#upper left arm\n",
        "# \tshould1_left, should2_left = qpos[13:15]\n",
        "# \tshould_left_quat = quat_from_xyz(x_angle = should1_left, y_angle =should2_left)\n",
        "# \tquaternions = jp.concatenate([quaternions, should_left_quat])\n",
        "\n",
        "\n",
        "# \t#left elbow\n",
        "# \telbow_left = qpos[15]\n",
        "# \telbow_left_quat = quat_from_xyz(y_angle = elbow_left)\n",
        "# \tquaternions = jp.concatenate([quaternions, elbow_left_quat])\n",
        "\n",
        "\n",
        "# \t#right thigh\n",
        "# \thip_x_right, hip_z_right, hip_y_right = qpos[16:19]\n",
        "# \thip_right_quat = quat_from_xyz(x_angle=hip_x_right, y_angle=hip_y_right, z_angle=hip_z_right)\n",
        "# \tquaternions = jp.concatenate([quaternions, hip_right_quat])\n",
        "\n",
        "\n",
        "# \t#right shin\n",
        "# \tknee_right = qpos[19]\n",
        "# \tknee_right_quat = quat_from_xyz(y_angle=knee_right)\n",
        "# \tquaternions = jp.concatenate([quaternions, knee_right_quat])\n",
        "\n",
        "\n",
        "# \t#right foot\n",
        "# \tankle_y_right, ankle_x_right = qpos[20:22]\n",
        "# \tankle_right_quat = quat_from_xyz(x_angle=ankle_x_right, y_angle=ankle_y_right)\n",
        "# \tquaternions = jp.concatenate([quaternions, ankle_right_quat])\n",
        "\n",
        "\n",
        "# \t#left thigh\n",
        "# \thip_x_left, hip_z_left, hip_y_left = qpos[22:25]\n",
        "# \thip_left_quat = quat_from_xyz(x_angle=hip_x_left * -1.0, y_angle=hip_y_left, z_angle=hip_z_left * -1.0)\n",
        "# \tquaternions = jp.concatenate([quaternions, hip_left_quat])\n",
        "\n",
        "\n",
        "# \t#left shin\n",
        "# \tknee_left = qpos[25]\n",
        "# \tknee_left_quat = quat_from_xyz(y_angle=knee_left)\n",
        "# \tquaternions = jp.concatenate([quaternions, knee_left_quat])\n",
        "\n",
        "\n",
        "# \t#left foot\n",
        "# \tankle_y_left, ankle_x_left = qpos[26:28]\n",
        "# \tankle_left_quat = quat_from_xyz(x_angle=ankle_x_left * -1.0, y_angle=ankle_y_left)\n",
        "# \tquaternions = jp.concatenate([quaternions, ankle_left_quat])\n",
        "\n",
        "# \tprint(\"QUATERNIONS LENGTH: \", len(quaternions))\n",
        "# \treturn quaternions\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "7Wz7oFqnU7Rl",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title View Mocap Only\n",
        "HUMANOID_ROOT_PATH = epath.Path(epath.resource_path('mujoco')) / 'mjx/test_data/humanoid'\n",
        "TERRAIN_ROOT_PATH = epath.Path('mujoco_resources/humanoid_terrain')\n",
        "FIXED_ROOT_PATH = epath.Path('mujoco_resources/humanoid_CMU_folder')\n",
        "from jax import lax\n",
        "\n",
        "\n",
        "class Humanoid(PipelineEnv):\n",
        "  def __init__(\n",
        "      self,\n",
        "      forward_reward_weight=1.25,\n",
        "      ctrl_cost_weight=0.1,\n",
        "      healthy_reward=2.0,\n",
        "      terminate_when_unhealthy=True,\n",
        "      healthy_z_range=(0.5, 2.0),\n",
        "      reset_noise_scale=1e-2,\n",
        "      exclude_current_positions_from_observation=True,\n",
        "      target_speed = 1.0,\n",
        "      heading_reward_weight= 5.0,\n",
        "      **kwargs,\n",
        "  ):\n",
        "\n",
        "    # Option 1: TRAIN WITHOUT HEIGHT FIELD-- plain humanoid\n",
        "    # mj_model = mujoco.MjModel.from_xml_path(\n",
        "    #     (HUMANOID_ROOT_PATH / 'humanoid.xml').as_posix())\n",
        "\n",
        "    # Option 2: TRAIN WITH HEIGH FIELD-- humanoid with terrain\n",
        "    # mj_model = mujoco.MjModel.from_xml_path(\n",
        "    #     (TERRAIN_ROOT_PATH / 'humanoid.xml').as_posix())\n",
        "    mj_model = mujoco.MjModel.from_xml_path(\n",
        "         (FIXED_ROOT_PATH / 'humanoid_fixed_7.xml').as_posix())\n",
        "\n",
        "    mj_model.opt.solver = mujoco.mjtSolver.mjSOL_CG\n",
        "    mj_model.opt.iterations = 6\n",
        "    mj_model.opt.ls_iterations = 6\n",
        "\n",
        "    sys = mjcf.load_model(mj_model)\n",
        "\n",
        "    physics_steps_per_control_step = 5\n",
        "\n",
        "    kwargs['n_frames'] = kwargs.get(\n",
        "        'n_frames', physics_steps_per_control_step)\n",
        "    kwargs['backend'] = 'mjx'\n",
        "\n",
        "    super().__init__(sys, **kwargs)\n",
        "\n",
        "    # Original Rewards/Costs\n",
        "    self._forward_reward_weight = forward_reward_weight\n",
        "    self._ctrl_cost_weight = ctrl_cost_weight\n",
        "    self._healthy_reward = healthy_reward\n",
        "    self._terminate_when_unhealthy = terminate_when_unhealthy\n",
        "    self._healthy_z_range = healthy_z_range\n",
        "    self._reset_noise_scale = reset_noise_scale\n",
        "    self._exclude_current_positions_from_observation = (\n",
        "        exclude_current_positions_from_observation\n",
        "    )\n",
        "\n",
        "    # New Parameters\n",
        "    self._target_speed = target_speed\n",
        "    self._heading_weight= heading_reward_weight\n",
        "\n",
        "    #mocap stuff\n",
        "    self.mocap_dict = parse_bvh_file()\n",
        "    self.frames = self.mocap_dict[\"martial_arts\"][0]\n",
        "    self.frames_qpos = []\n",
        "    for frame in self.frames:\n",
        "      qpos = frame[\"qpos\"]\n",
        "      self.frames_qpos.append(qpos)\n",
        "    #each row is a frame\n",
        "    self.frames_qpos = np.array(self.frames_qpos)\n",
        "    print(self.frames_qpos. shape[0], \", \", self.frames_qpos.shape[1])\n",
        "    self.frames_qpos = jp.array(self.frames_qpos)\n",
        "    self.step_count = 0\n",
        "\n",
        "  def reset(self, rng: jp.ndarray) -> State:\n",
        "    \"\"\"Resets the environment to an initial state.\"\"\"\n",
        "    self.step_count = 0\n",
        "    rng, rng1, rng2, rng3 = jax.random.split(rng, 4)\n",
        "\n",
        "    low, hi = -self._reset_noise_scale, self._reset_noise_scale\n",
        "    qpos = self.sys.qpos0 + jax.random.uniform(\n",
        "        rng1, (self.sys.nq,), minval=low, maxval=hi\n",
        "    )\n",
        "    qvel = jax.random.uniform(\n",
        "        rng2, (self.sys.nv,), minval=low, maxval=hi\n",
        "    )\n",
        "\n",
        "    data = self.pipeline_init(qpos, qvel)\n",
        "\n",
        "    theta = jax.random.uniform(rng3, (), minval=0.0, maxval=2 * jp.pi)\n",
        "    d_star = jp.stack([jp.cos(theta), jp.sin(theta)])\n",
        "\n",
        "    state_info = {\n",
        "        'rng3': rng3,\n",
        "        'goal': d_star,\n",
        "        'v_xy': jp.zeros((2,), dtype=jp.float32),\n",
        "        'frame_count': jp.array(0),\n",
        "    }\n",
        "\n",
        "    obs = self._get_obs(data, jp.zeros(self.sys.nu), state_info)\n",
        "    reward, done, zero = jp.zeros(3)\n",
        "    metrics = {\n",
        "        'forward_reward': zero,\n",
        "        'reward_linvel': zero,\n",
        "        'reward_quadctrl': zero,\n",
        "        'reward_alive': zero,\n",
        "        'x_position': zero,\n",
        "        'y_position': zero,\n",
        "        'distance_from_origin': zero,\n",
        "        'x_velocity': zero,\n",
        "        'y_velocity': zero,\n",
        "    }\n",
        "\n",
        "    state = State(data, obs, reward, done, metrics, state_info)\n",
        "\n",
        "    # Sample a random 2D unit vector for the desired heading\n",
        "    # theta = jax.random.uniform(rng3, (), minval=0.0, maxval=2 * jp.pi)\n",
        "    # d_star = jp.stack([jp.cos(theta), jp.sin(theta)])\n",
        "    # state.info['goal'] = d_star\n",
        "    return state\n",
        "\n",
        "  def imit_reward(self, mo_pos, stt_pos):\n",
        "\n",
        "    mo_pos = jp.array(mo_pos)\n",
        "    stt_pos = jp.array(stt_pos)\n",
        "\n",
        "    pose_reward = 0\n",
        "    vel_reward = 0\n",
        "    #list of floats representing values in quaternion rotations\n",
        "    mo_rot_data = mo_pos[7:]\n",
        "    stt_rot_data = stt_pos[7:]\n",
        "\n",
        "    print(\"mo_rot_data: \", len(mo_rot_data))\n",
        "    print(\"stt_rot_data: \", len(stt_rot_data))\n",
        "\n",
        "    for r in range(0, len(mo_rot_data)):\n",
        "        rot_diff = mo_rot_data[r] - stt_rot_data[r]\n",
        "        pose_reward += (rot_diff ** 2)\n",
        "\n",
        "        #find difference between mocap rotation for given joint and the state rotation\n",
        "        #make negative so that when we raise e to it, higher difference leads to smaller reward\n",
        "        #pose_reward += (diff_quat) ** 2.0\n",
        "\n",
        "\n",
        "\n",
        "    #multiply by negative number so greater difference = more negative which means smaller fraction e is raised to it\n",
        "    #maxes out when there's no difference and reward ends up being 1\n",
        "    pose_reward *= -2.0\n",
        "\n",
        "    pose_reward = jp.exp(jp.maximum(pose_reward, -50))\n",
        "\n",
        "\n",
        "\n",
        "    # mo_ang_vel = mo_vel[3:]\n",
        "    # stt_ang_vel = stt_vel[:]\n",
        "\n",
        "\n",
        "    # for v in range(0, len(mo_ang_vel)):\n",
        "    #     vel_diff = mo_ang_vel[v] - stt_ang_vel[v]\n",
        "    #     vel_reward += (vel_diff) ** 2.0\n",
        "\n",
        "    # vel_reward *= -0.1\n",
        "    # vel_reward = math.exp(max(vel_reward, -50))\n",
        "\n",
        "\n",
        "    #first 3 values in both lists represent xyz pos of hips\n",
        "    # center_mass_reward = jp.linalg.norm(mo_pos[:3] - stt_pos[:3])\n",
        "    # center_mass_reward *= -10.0\n",
        "    # center_mass_reward = jp.exp(jp.maximum(center_mass_reward, -50))\n",
        "\n",
        "    #full_reward = (0.7 * pose_reward) + (0.3 * center_mass_reward)\n",
        "    full_reward = pose_reward\n",
        "    return full_reward\n",
        "\n",
        "\n",
        "  def step(self, state: State, action: jp.ndarray) -> State:\n",
        "    current_frame = state.info['frame_count']\n",
        "    print(\"Current Frame:\", current_frame)\n",
        "    \"\"\"Runs one timestep of the environment's dynamics.\"\"\"\n",
        "    data0 = state.pipeline_state\n",
        "    #data = self.pipeline_step(data0, action)\n",
        "\n",
        "    #FOR DEBUGGING MOCAP DATA\n",
        "    data = data0\n",
        "    hip_pos = data.qpos[:7]\n",
        "    rest = self.frames_qpos[current_frame]\n",
        "\n",
        "    new_qpos = jp.concatenate([hip_pos, rest])\n",
        "    data = self.pipeline_init(rest, data.qvel)\n",
        "\n",
        "    # com_before = data0.subtree_com[1]\n",
        "    # com_after = data.subtree_com[1]\n",
        "    # # velocity = (x_t - x_0) / t\n",
        "    # velocity = (com_after - com_before) / self.dt\n",
        "\n",
        "    # # New Reward: Target Heading Task Reward\n",
        "    # d_star    = state.info['goal']\n",
        "    # v_xy   = velocity[:2] @ d_star # <--- v_xy = [v_x, v_y] @ [d_x, d_y]\n",
        "    # speed_err = jp.maximum(0.0, self._target_speed - v_xy)\n",
        "    # heading_r = jp.exp(-2.5 * speed_err**2)\n",
        "    # forward_reward = self._heading_weight * heading_r\n",
        "    # state.info['v_xy'] = velocity[:2]\n",
        "\n",
        "    # Old Reward -- velocity[0] is x component of velocity {vx, vy, vz}\n",
        "    #forward_reward = self._forward_reward_weight * velocity[0]\n",
        "    #forward_reward = 1.25 * velocity in the x direction\n",
        "\n",
        "    # min_z, max_z = self._healthy_z_range\n",
        "    # is_healthy = jp.where(data.q[2] < min_z, 0.0, 1.0)\n",
        "    # is_healthy = jp.where(data.q[2] > max_z, 0.0, is_healthy)\n",
        "    # if self._terminate_when_unhealthy:\n",
        "    #   healthy_reward = self._healthy_reward\n",
        "    # else:\n",
        "    #   healthy_reward = self._healthy_reward * is_healthy\n",
        "\n",
        "    # ctrl_cost = self._ctrl_cost_weight * jp.sum(jp.square(action))\n",
        "    frame_count = state.info['frame_count'] + 1\n",
        "    state_info = dict(state.info)\n",
        "    state_info['frame_count'] = frame_count\n",
        "\n",
        "    obs = self._get_obs(data, action, state.info)\n",
        "\n",
        "    #IMITATION REWARD!!!!!!\n",
        "    # max_frame = self.frames_qpos.shape[0] - 1\n",
        "\n",
        "    # def compute_reward(_):\n",
        "    #     mocap_qpos = self.frames_qpos[current_frame]\n",
        "    #     return 3.0 * self.imit_reward(mocap_qpos, data.qpos)\n",
        "\n",
        "    # def zero_reward(_):\n",
        "    #     return 0.0\n",
        "\n",
        "    # imit_reward = lax.cond(\n",
        "    #     current_frame < max_frame,\n",
        "    #     compute_reward,\n",
        "    #     zero_reward,\n",
        "    #     operand=None\n",
        "    # )\n",
        "    # if current_frame < (self.frames_qpos.shape[0] - 1):\n",
        "    #   mocap_qpos = self.frames_qpos[current_frame]\n",
        "\n",
        "    #   imit_reward = 3.0 * self.imit_reward(mocap_qpos, data.qpos)\n",
        "    # else:\n",
        "    #   imit_reward = 0\n",
        "\n",
        "\n",
        "    #ADD IT ALL UP\n",
        "    # reward = forward_reward + healthy_reward - ctrl_cost\n",
        "\n",
        "    # done = 1.0 - is_healthy if self._terminate_when_unhealthy else 0.0\n",
        "    # state.metrics.update(\n",
        "    #     forward_reward=forward_reward,\n",
        "    #     reward_linvel=forward_reward,\n",
        "    #     reward_quadctrl=-ctrl_cost,\n",
        "    #     reward_alive=healthy_reward,\n",
        "    #     x_position=com_after[0],\n",
        "    #     y_position=com_after[1],\n",
        "    #     distance_from_origin=jp.linalg.norm(com_after),\n",
        "    #     x_velocity=velocity[0],\n",
        "    #     y_velocity=velocity[1],\n",
        "    # )\n",
        "\n",
        "    reward = 1\n",
        "    done = False\n",
        "\n",
        "    return state.replace(\n",
        "        pipeline_state=data, obs=obs, reward=reward, done=done, info=state_info\n",
        "    )\n",
        "\n",
        "  def _get_obs(\n",
        "      self, data: mjx.Data, action: jp.ndarray, state_info: dict[str, Any],\n",
        "  ) -> jp.ndarray:\n",
        "    \"\"\"Observes humanoid body position, velocities, and angles.\"\"\"\n",
        "\n",
        "    position = data.qpos\n",
        "    if self._exclude_current_positions_from_observation:\n",
        "      position = position[2:]\n",
        "\n",
        "    # print(\"Print data.qpos in _get_obs:\", data.qpos)\n",
        "    # print(\"Print data.qvel in _get_obs:\", data.qvel)\n",
        "    # # mass and inertia tensor in the center of mass (COM) frame.\n",
        "    # print(\"Print flattened data.cinert in _get_obs:\", data.cinert.ravel())\n",
        "    # print(\"Print flattened data.cvel in _get_obs:\", data.cvel.ravel())\n",
        "    # print(\"Print data.qfrc_actuator in _get_obs:\", data.qfrc_actuator)\n",
        "\n",
        "    # external_contact_forces are excluded\n",
        "    return jp.concatenate([\n",
        "        position,\n",
        "        data.qvel,\n",
        "        data.cinert[1:].ravel(),\n",
        "        data.cvel[1:].ravel(),\n",
        "        data.qfrc_actuator,\n",
        "\n",
        "        # state_info['goal'],\n",
        "        # state_info['v_xy'],\n",
        "    ])\n",
        "\n",
        "\n",
        "# register env class we just made\n",
        "envs.register_environment('humanoid', Humanoid)\n",
        "\n",
        "# set the env as this humanoid env\n",
        "env_name = 'humanoid'\n",
        "env = envs.get_environment(env_name)\n",
        "\n",
        "# define the jit reset/step functions to put it on the GPU\n",
        "jit_reset = jax.jit(env.reset)\n",
        "jit_step = jax.jit(env.step)"
      ],
      "metadata": {
        "id": "2RgavHlvvYnI",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Define Humanoid Env for Good Walk\n",
        "HUMANOID_ROOT_PATH = epath.Path(epath.resource_path('mujoco')) / 'mjx/test_data/humanoid'\n",
        "TERRAIN_ROOT_PATH = epath.Path('mujoco_resources/humanoid_terrain')\n",
        "FIXED_ROOT_PATH = epath.Path('mujoco_resources/humanoid_CMU_folder')\n",
        "from jax import lax\n",
        "from jax import random\n",
        "\n",
        "\n",
        "def quat_inv(q):\n",
        "    \"\"\"Returns the inverse (conjugate) of a unit quaternion [w, x, y, z].\"\"\"\n",
        "    w, x, y, z = q\n",
        "    return jp.array([w, -x, -y, -z])\n",
        "\n",
        "def quat_mul(q1, q2):\n",
        "        \"\"\"Returns the multiplication of two quaternions [w, x, y, z].\"\"\"\n",
        "        w1, x1, y1, z1 = q1\n",
        "        w2, x2, y2, z2 = q2\n",
        "        return jp.array([\n",
        "            w1*w2 - x1*x2 - y1*y2 - z1*z2,\n",
        "            w1*x2 + x1*w2 + y1*z2 - z1*y2,\n",
        "            w1*y2 - x1*z2 + y1*w2 + z1*x2,\n",
        "            w1*z2 + x1*y2 - y1*x2 + z1*w2\n",
        "        ])\n",
        "\n",
        "def quat_to_rotvec(q):\n",
        "        \"\"\"Convert a unit quaternion [w, x, y, z] to axis-angle vector (rotvec).\"\"\"\n",
        "        norm_q = q / jp.linalg.norm(q)\n",
        "        w, xyz = norm_q[0], norm_q[1:]\n",
        "        sin_half_theta = jp.linalg.norm(xyz)\n",
        "        angle = 2.0 * jp.arctan2(sin_half_theta, w)\n",
        "        axis = jp.where(sin_half_theta > 1e-6, xyz / sin_half_theta, jp.zeros_like(xyz))\n",
        "        return angle * axis\n",
        "\n",
        "class Humanoid(PipelineEnv):\n",
        "  def __init__(\n",
        "      self,\n",
        "      forward_reward_weight=1.25,\n",
        "      ctrl_cost_weight=0.1,\n",
        "      healthy_reward=2.0,\n",
        "      terminate_when_unhealthy=True,\n",
        "      healthy_z_range=(0.5, 2.0),\n",
        "      reset_noise_scale=1e-2,\n",
        "      exclude_current_positions_from_observation=True,\n",
        "      target_speed = 1.0,\n",
        "      heading_reward_weight= 5.0,\n",
        "      **kwargs,\n",
        "  ):\n",
        "\n",
        "    # Option 1: TRAIN WITHOUT HEIGHT FIELD-- plain humanoid\n",
        "    # mj_model = mujoco.MjModel.from_xml_path(\n",
        "    #     (HUMANOID_ROOT_PATH / 'humanoid.xml').as_posix())\n",
        "\n",
        "    # Option 2: TRAIN WITH HEIGH FIELD-- humanoid with terrain\n",
        "    # mj_model = mujoco.MjModel.from_xml_path(\n",
        "    #     (TERRAIN_ROOT_PATH / 'humanoid.xml').as_posix())\n",
        "    mj_model = mujoco.MjModel.from_xml_path(\n",
        "         (FIXED_ROOT_PATH / 'humanoid_fixed_7.xml').as_posix())\n",
        "\n",
        "    mj_model.opt.solver = mujoco.mjtSolver.mjSOL_CG\n",
        "    mj_model.opt.iterations = 6\n",
        "    mj_model.opt.ls_iterations = 6\n",
        "\n",
        "    sys = mjcf.load_model(mj_model)\n",
        "\n",
        "    physics_steps_per_control_step = 5\n",
        "\n",
        "    kwargs['n_frames'] = kwargs.get(\n",
        "        'n_frames', physics_steps_per_control_step)\n",
        "    kwargs['backend'] = 'mjx'\n",
        "\n",
        "    super().__init__(sys, **kwargs)\n",
        "\n",
        "    # Original Rewards/Costs\n",
        "    self._forward_reward_weight = forward_reward_weight\n",
        "    self._ctrl_cost_weight = ctrl_cost_weight\n",
        "    self._healthy_reward = healthy_reward\n",
        "    self._terminate_when_unhealthy = terminate_when_unhealthy\n",
        "    self._healthy_z_range = healthy_z_range\n",
        "    self._reset_noise_scale = reset_noise_scale\n",
        "    self._exclude_current_positions_from_observation = (\n",
        "        exclude_current_positions_from_observation\n",
        "    )\n",
        "\n",
        "    # New Parameters\n",
        "    self._target_speed = target_speed\n",
        "    self._heading_weight= heading_reward_weight\n",
        "\n",
        "    #mocap stuff\n",
        "    self.mocap_dict = parse_bvh_file()\n",
        "    self.frames = self.mocap_dict[\"walking\"][0]\n",
        "    self.frames_qvel = []\n",
        "    self.frames_qpos = []\n",
        "    for frame in self.frames:\n",
        "      qpos = frame[\"qpos\"]\n",
        "      qvel = frame[\"qvel\"]\n",
        "      self.frames_qpos.append(qpos)\n",
        "      self.frames_qvel.append(qvel)\n",
        "    #each row is a frame\n",
        "    self.frames_qpos = np.array(self.frames_qpos)\n",
        "    self.frames_qvel = np.array(self.frames_qvel)\n",
        "    print(self.frames_qpos. shape[0], \", \", self.frames_qpos.shape[1])\n",
        "    self.frames_qpos = jp.array(self.frames_qpos)\n",
        "    self.frames_qvel = jp.array(self.frames_qvel)\n",
        "    self.step_count = 0\n",
        "    self.key = random.PRNGKey(42)  # Set the seed ONCE\n",
        "    self.key, subkey = random.split(self.key)\n",
        "    self.random_frame = random.randint(subkey, shape=(), minval=0, maxval=4000)\n",
        "    print(\"rand frame: \", self.random_frame)\n",
        "\n",
        "  def reset(self, rng: jp.ndarray) -> State:\n",
        "    \"\"\"Resets the environment to an initial state.\"\"\"\n",
        "    # key = random.PRNGKey(rng3)  # Set the seed\n",
        "    # key, subkey = random.split(self.key)\n",
        "    rng, rng1, rng2, rng3 = jax.random.split(rng, 4)\n",
        "    random_frame = random.randint(rng1, shape=(), minval=0, maxval=4000)\n",
        "    #jax.debug.print(\"Random frame: {}\", random_frame)\n",
        "\n",
        "    low, hi = -self._reset_noise_scale, self._reset_noise_scale\n",
        "    qpos = self.sys.qpos0 + jax.random.uniform(\n",
        "        rng1, (self.sys.nq,), minval=low, maxval=hi\n",
        "    )\n",
        "    qvel = jax.random.uniform(\n",
        "        rng2, (self.sys.nv,), minval=low, maxval=hi\n",
        "    )\n",
        "\n",
        "    #RANDOMLY INITIALIZE TO SOME RANDOM FRAME IN THE MOCAP\n",
        "    qpos = qpos.at[7:].set(self.frames_qpos[random_frame][7:])\n",
        "    qvel = qvel.at[6:].set(self.frames_qvel[random_frame][6:])\n",
        "\n",
        "    data = self.pipeline_init(qpos, qvel)\n",
        "\n",
        "    theta = jax.random.uniform(rng3, (), minval=0.0, maxval=2 * jp.pi)\n",
        "    d_star = jp.stack([jp.cos(theta), jp.sin(theta)])\n",
        "\n",
        "    state_info = {\n",
        "        'rng3': rng3,\n",
        "        'goal': d_star,\n",
        "        'v_xy': jp.zeros((2,), dtype=jp.float32),\n",
        "        'frame_count': jp.array(random_frame),\n",
        "        'mocap_qpos': qpos,\n",
        "        'mocap_qvel': qvel\n",
        "    }\n",
        "\n",
        "    obs = self._get_obs(data, jp.zeros(self.sys.nu), state_info)\n",
        "    reward, done, zero = jp.zeros(3)\n",
        "    metrics = {\n",
        "        'forward_reward': zero,\n",
        "        'reward_linvel': zero,\n",
        "        'reward_quadctrl': zero,\n",
        "        'reward_alive': zero,\n",
        "        'x_position': zero,\n",
        "        'y_position': zero,\n",
        "        'distance_from_origin': zero,\n",
        "        'x_velocity': zero,\n",
        "        'y_velocity': zero,\n",
        "    }\n",
        "\n",
        "    state = State(data, obs, reward, done, metrics, state_info)\n",
        "\n",
        "    # Sample a random 2D unit vector for the desired heading\n",
        "    # theta = jax.random.uniform(rng3, (), minval=0.0, maxval=2 * jp.pi)\n",
        "    # d_star = jp.stack([jp.cos(theta), jp.sin(theta)])\n",
        "    # state.info['goal'] = d_star\n",
        "    return state\n",
        "\n",
        "  def imit_reward(self, mo_pos, stt_pos, mo_vel, stt_vel):\n",
        "\n",
        "    mo_pos = jp.array(mo_pos)\n",
        "    stt_pos = jp.array(stt_pos)\n",
        "\n",
        "    pose_reward = 0\n",
        "    vel_reward = 0\n",
        "\n",
        "    # rot_diff = quat_mul(mo_pos[3:7], quat_inv(stt_pos[3:7]))\n",
        "    # diff_angle = quat_to_rotvec(rot_diff)\n",
        "    # pose_reward = jp.sum(diff_angle**2)\n",
        "    #list of floats representing values in quaternion rotations\n",
        "    mo_rot_data = mo_pos[7:]\n",
        "    stt_rot_data = stt_pos[7:]\n",
        "\n",
        "    print(\"mo_rot_data: \", len(mo_rot_data))\n",
        "    print(\"stt_rot_data: \", len(stt_rot_data))\n",
        "\n",
        "    for r in range(0, len(mo_rot_data)):\n",
        "        rot_diff = mo_rot_data[r] - stt_rot_data[r]\n",
        "        pose_reward += (rot_diff ** 2)\n",
        "\n",
        "        #find difference between mocap rotation for given joint and the state rotation\n",
        "        #make negative so that when we raise e to it, higher difference leads to smaller reward\n",
        "        #pose_reward += (diff_quat) ** 2.0\n",
        "\n",
        "\n",
        "\n",
        "    #multiply by negative number so greater difference = more negative which means smaller fraction e is raised to it\n",
        "    #maxes out when there's no difference and reward ends up being 1\n",
        "    pose_reward *= -2.0\n",
        "\n",
        "    pose_reward = jp.exp(jp.maximum(pose_reward, -50))\n",
        "\n",
        "\n",
        "\n",
        "    mo_ang_vel = mo_vel[6:]\n",
        "    stt_ang_vel = stt_vel[6:]\n",
        "\n",
        "\n",
        "    for v in range(0, len(mo_ang_vel)):\n",
        "       vel_diff = mo_ang_vel[v] - stt_ang_vel[v]\n",
        "       vel_reward += (vel_diff ** 2.0)\n",
        "\n",
        "    vel_reward *= -0.1\n",
        "    vel_reward = jp.exp(jp.maximum(vel_reward, -50))\n",
        "\n",
        "\n",
        "    #first 3 values in both lists represent xyz pos of hips\n",
        "    # center_mass_reward = jp.linalg.norm(mo_pos[:3] - stt_pos[:3])\n",
        "    # center_mass_reward *= -10.0\n",
        "    # center_mass_reward = jp.exp(jp.maximum(center_mass_reward, -50))\n",
        "\n",
        "    #full_reward = (0.7 * pose_reward) + (0.3 * center_mass_reward)\n",
        "    full_reward = 0.7 * pose_reward + 0.3 * vel_reward\n",
        "    return full_reward\n",
        "\n",
        "\n",
        "\n",
        "  def step(self, state: State, action: jp.ndarray) -> State:\n",
        "    current_frame = state.info['frame_count']\n",
        "    print(\"Current Frame:\", current_frame)\n",
        "    \"\"\"Runs one timestep of the environment's dynamics.\"\"\"\n",
        "    data0 = state.pipeline_state\n",
        "    data = self.pipeline_step(data0, action)\n",
        "\n",
        "    #FOR DEBUGGING MOCAP DATA\n",
        "    # data = data0\n",
        "    # hip_pos = data.qpos[:7]\n",
        "    # rest = self.frames_qpos[current_frame][7:]\n",
        "    # new_qpos = jp.concatenate([hip_pos, rest])\n",
        "    # data = self.pipeline_init(new_qpos, data.qvel)\n",
        "\n",
        "    com_before = data0.subtree_com[1]\n",
        "    com_after = data.subtree_com[1]\n",
        "    # velocity = (x_t - x_0) / t\n",
        "    velocity = (com_after - com_before) / self.dt\n",
        "\n",
        "    # # New Reward: Target Heading Task Reward\n",
        "    # d_star    = state.info['goal']\n",
        "    # v_xy   = velocity[:2] @ d_star # <--- v_xy = [v_x, v_y] @ [d_x, d_y]\n",
        "    # speed_err = jp.maximum(0.0, self._target_speed - v_xy)\n",
        "    # heading_r = jp.exp(-2.5 * speed_err**2)\n",
        "    # forward_reward = self._heading_weight * heading_r\n",
        "    # state.info['v_xy'] = velocity[:2]\n",
        "\n",
        "    # Old Reward -- velocity[0] is x component of velocity {vx, vy, vz}\n",
        "    forward_reward = self._forward_reward_weight * (velocity/jp.linalg.norm(velocity))[0] #forward_reward = 1.25 * velocity in the x direction\n",
        "\n",
        "    min_z, max_z = self._healthy_z_range\n",
        "    is_healthy = jp.where(data.q[2] < min_z, 0.0, 1.0)\n",
        "    is_healthy = jp.where(data.q[2] > max_z, 0.0, is_healthy)\n",
        "    is_healthy = jp.where(data.xpos[2][2] < (min_z + .3), 0.0, is_healthy)\n",
        "    is_healthy = jp.where(data.xpos[2][2] > (max_z + .3), 0.0, is_healthy)\n",
        "    if self._terminate_when_unhealthy:\n",
        "      healthy_reward = self._healthy_reward\n",
        "    else:\n",
        "      healthy_reward = self._healthy_reward * is_healthy\n",
        "\n",
        "    ctrl_cost = self._ctrl_cost_weight * jp.sum(jp.square(action))\n",
        "    frame_count = state.info['frame_count'] + 1\n",
        "    state_info = dict(state.info)\n",
        "    state_info['frame_count'] = frame_count\n",
        "\n",
        "    obs = self._get_obs(data, action, state.info)\n",
        "\n",
        "    #IMITATION REWARD!!!!!!\n",
        "    max_frame = self.frames_qpos.shape[0] - 1\n",
        "\n",
        "    mocap_qpos = self.frames_qpos[0]\n",
        "    mocap_qvel = self.frames_qvel[0]\n",
        "    mocap_qpos = jp.zeros_like(mocap_qpos)\n",
        "    mocap_qvel = jp.zeros_like(mocap_qvel)\n",
        "    def compute_reward(_):\n",
        "        mocap_qpos = self.frames_qpos[current_frame]\n",
        "        mocap_qvel = self.frames_qvel[current_frame]\n",
        "        return self.imit_reward(mocap_qpos, data.qpos, mocap_qvel, data.qvel)\n",
        "\n",
        "    def zero_reward(_):\n",
        "        return 0.0\n",
        "\n",
        "    imit_reward = lax.cond(\n",
        "        current_frame < max_frame,\n",
        "        compute_reward,\n",
        "        zero_reward,\n",
        "        operand=None\n",
        "    )\n",
        "\n",
        "    state.info['mocap_qpos'] = mocap_qpos\n",
        "    state.info['mocap_qvel'] = mocap_qvel\n",
        "\n",
        "    #GIVE IT SOME REWARD FOR NOT BENDING IT'S TORSO MUCH\n",
        "    torso_rot = data.qpos[7:10]\n",
        "    stiffness_reward = 0\n",
        "    for r in torso_rot:\n",
        "      stiffness_reward += r**2\n",
        "    stiffness_reward = jp.exp(stiffness_reward * -2)\n",
        "\n",
        "\n",
        "\n",
        "    #ADD IT ALL UP\n",
        "    reward = forward_reward + healthy_reward + stiffness_reward - ctrl_cost\n",
        "\n",
        "    done = 1.0 - is_healthy if self._terminate_when_unhealthy else 0.0\n",
        "    state.metrics.update(\n",
        "        forward_reward=forward_reward,\n",
        "        reward_linvel=forward_reward,\n",
        "        reward_quadctrl=-ctrl_cost,\n",
        "        reward_alive=healthy_reward,\n",
        "        x_position=com_after[0],\n",
        "        y_position=com_after[1],\n",
        "        distance_from_origin=jp.linalg.norm(com_after),\n",
        "        x_velocity=velocity[0],\n",
        "        y_velocity=velocity[1],\n",
        "    )\n",
        "\n",
        "\n",
        "    return state.replace(\n",
        "        pipeline_state=data, obs=obs, reward=reward, done=done, info=state_info\n",
        "    )\n",
        "\n",
        "  def _get_obs(\n",
        "      self, data: mjx.Data, action: jp.ndarray, state_info: dict[str, Any],\n",
        "  ) -> jp.ndarray:\n",
        "    \"\"\"Observes humanoid body position, velocities, and angles.\"\"\"\n",
        "\n",
        "    position = data.qpos\n",
        "    if self._exclude_current_positions_from_observation:\n",
        "      position = position[2:]\n",
        "\n",
        "    # print(\"Print data.qpos in _get_obs:\", data.qpos)\n",
        "    # print(\"Print data.qvel in _get_obs:\", data.qvel)\n",
        "    # # mass and inertia tensor in the center of mass (COM) frame.\n",
        "    # print(\"Print flattened data.cinert in _get_obs:\", data.cinert.ravel())\n",
        "    # print(\"Print flattened data.cvel in _get_obs:\", data.cvel.ravel())\n",
        "    # print(\"Print data.qfrc_actuator in _get_obs:\", data.qfrc_actuator)\n",
        "\n",
        "    # external_contact_forces are excluded\n",
        "    return jp.concatenate([\n",
        "        position,\n",
        "        data.qvel,\n",
        "        data.cinert[1:].ravel(),\n",
        "        data.cvel[1:].ravel(),\n",
        "        data.qfrc_actuator,\n",
        "        state_info['mocap_qpos'],\n",
        "        state_info['mocap_qvel']\n",
        "        # state_info['goal'],\n",
        "        # state_info['v_xy'],\n",
        "    ])\n",
        "\n",
        "\n",
        "# register env class we just made\n",
        "envs.register_environment('humanoid', Humanoid)\n",
        "\n",
        "# set the env as this humanoid env\n",
        "env_name = 'humanoid'\n",
        "env = envs.get_environment(env_name)\n",
        "\n",
        "# define the jit reset/step functions to put it on the GPU\n",
        "jit_reset = jax.jit(env.reset)\n",
        "jit_step = jax.jit(env.step)"
      ],
      "metadata": {
        "id": "5VY2GvV8ejkF",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Define Humanoid Env for Walking in Circle\n",
        "HUMANOID_ROOT_PATH = epath.Path(epath.resource_path('mujoco')) / 'mjx/test_data/humanoid'\n",
        "FIXED_ROOT_PATH = epath.Path('mujoco_resources/humanoid_CMU_folder')\n",
        "from jax import lax\n",
        "from jax import random\n",
        "\n",
        "\n",
        "def quat_inv(q):\n",
        "    \"\"\"Returns the inverse (conjugate) of a unit quaternion [w, x, y, z].\"\"\"\n",
        "    w, x, y, z = q\n",
        "    return jp.array([w, -x, -y, -z])\n",
        "\n",
        "def quat_mul(q1, q2):\n",
        "        \"\"\"Returns the multiplication of two quaternions [w, x, y, z].\"\"\"\n",
        "        w1, x1, y1, z1 = q1\n",
        "        w2, x2, y2, z2 = q2\n",
        "        return jp.array([\n",
        "            w1*w2 - x1*x2 - y1*y2 - z1*z2,\n",
        "            w1*x2 + x1*w2 + y1*z2 - z1*y2,\n",
        "            w1*y2 - x1*z2 + y1*w2 + z1*x2,\n",
        "            w1*z2 + x1*y2 - y1*x2 + z1*w2\n",
        "        ])\n",
        "\n",
        "def quat_to_rotvec(q):\n",
        "        \"\"\"Convert a unit quaternion [w, x, y, z] to axis-angle vector (rotvec).\"\"\"\n",
        "        norm_q = q / jp.linalg.norm(q)\n",
        "        w, xyz = norm_q[0], norm_q[1:]\n",
        "        sin_half_theta = jp.linalg.norm(xyz)\n",
        "        angle = 2.0 * jp.arctan2(sin_half_theta, w)\n",
        "        axis = jp.where(sin_half_theta > 1e-6, xyz / sin_half_theta, jp.zeros_like(xyz))\n",
        "        return angle * axis\n",
        "\n",
        "class Humanoid(PipelineEnv):\n",
        "  def __init__(\n",
        "      self,\n",
        "      forward_reward_weight=1.25,\n",
        "      ctrl_cost_weight=0.1,\n",
        "      healthy_reward=2.0,\n",
        "      terminate_when_unhealthy=True,\n",
        "      healthy_z_range=(0.5, 2.0),\n",
        "      reset_noise_scale=1e-2,\n",
        "      exclude_current_positions_from_observation=True,\n",
        "      target_speed = 1.0,\n",
        "      heading_reward_weight= 5.0,\n",
        "      **kwargs,\n",
        "  ):\n",
        "\n",
        "    # Option 1: TRAIN WITHOUT HEIGHT FIELD-- plain humanoid\n",
        "    # mj_model = mujoco.MjModel.from_xml_path(\n",
        "    #     (HUMANOID_ROOT_PATH / 'humanoid.xml').as_posix())\n",
        "\n",
        "    # Option 2: TRAIN WITH HEIGH FIELD-- humanoid with terrain\n",
        "    # mj_model = mujoco.MjModel.from_xml_path(\n",
        "    #     (TERRAIN_ROOT_PATH / 'humanoid.xml').as_posix())\n",
        "    mj_model = mujoco.MjModel.from_xml_path(\n",
        "         (FIXED_ROOT_PATH / 'humanoid_fixed_7.xml').as_posix())\n",
        "\n",
        "    mj_model.opt.solver = mujoco.mjtSolver.mjSOL_CG\n",
        "    mj_model.opt.iterations = 6\n",
        "    mj_model.opt.ls_iterations = 6\n",
        "\n",
        "    sys = mjcf.load_model(mj_model)\n",
        "\n",
        "    physics_steps_per_control_step = 5\n",
        "\n",
        "    kwargs['n_frames'] = kwargs.get(\n",
        "        'n_frames', physics_steps_per_control_step)\n",
        "    kwargs['backend'] = 'mjx'\n",
        "\n",
        "    super().__init__(sys, **kwargs)\n",
        "\n",
        "    # Original Rewards/Costs\n",
        "    self._forward_reward_weight = forward_reward_weight\n",
        "    self._ctrl_cost_weight = ctrl_cost_weight\n",
        "    self._healthy_reward = healthy_reward\n",
        "    self._terminate_when_unhealthy = terminate_when_unhealthy\n",
        "    self._healthy_z_range = healthy_z_range\n",
        "    self._reset_noise_scale = reset_noise_scale\n",
        "    self._exclude_current_positions_from_observation = (\n",
        "        exclude_current_positions_from_observation\n",
        "    )\n",
        "\n",
        "    # New Parameters\n",
        "    self._target_speed = target_speed\n",
        "    self._heading_weight= heading_reward_weight\n",
        "\n",
        "    #mocap stuff\n",
        "    self.mocap_dict = parse_bvh_file()\n",
        "    self.frames = self.mocap_dict[\"walking\"][0]\n",
        "    self.frames_qvel = []\n",
        "    self.frames_qpos = []\n",
        "    for frame in self.frames:\n",
        "      qpos = frame[\"qpos\"]\n",
        "      qvel = frame[\"qvel\"]\n",
        "      self.frames_qpos.append(qpos)\n",
        "      self.frames_qvel.append(qvel)\n",
        "    #each row is a frame\n",
        "    self.frames_qpos = np.array(self.frames_qpos)\n",
        "    self.frames_qvel = np.array(self.frames_qvel)\n",
        "    print(self.frames_qpos. shape[0], \", \", self.frames_qpos.shape[1])\n",
        "    self.frames_qpos = jp.array(self.frames_qpos)\n",
        "    self.frames_qvel = jp.array(self.frames_qvel)\n",
        "    self.step_count = 0\n",
        "    self.key = random.PRNGKey(42)  # Set the seed ONCE\n",
        "    self.key, subkey = random.split(self.key)\n",
        "    self.random_frame = random.randint(subkey, shape=(), minval=0, maxval=4000)\n",
        "    print(\"rand frame: \", self.random_frame)\n",
        "\n",
        "  def reset(self, rng: jp.ndarray) -> State:\n",
        "    \"\"\"Resets the environment to an initial state.\"\"\"\n",
        "    # key = random.PRNGKey(rng3)  # Set the seed\n",
        "    # key, subkey = random.split(self.key)\n",
        "    rng, rng1, rng2, rng3 = jax.random.split(rng, 4)\n",
        "    random_frame = random.randint(rng1, shape=(), minval=0, maxval=4000)\n",
        "    #jax.debug.print(\"Random frame: {}\", random_frame)\n",
        "\n",
        "    low, hi = -self._reset_noise_scale, self._reset_noise_scale\n",
        "    qpos = self.sys.qpos0 + jax.random.uniform(\n",
        "        rng1, (self.sys.nq,), minval=low, maxval=hi\n",
        "    )\n",
        "    qvel = jax.random.uniform(\n",
        "        rng2, (self.sys.nv,), minval=low, maxval=hi\n",
        "    )\n",
        "\n",
        "    #RANDOMLY INITIALIZE TO SOME RANDOM FRAME IN THE MOCAP\n",
        "    qpos = qpos.at[:].set(self.frames_qpos[random_frame][:])\n",
        "    qvel = qvel.at[:].set(self.frames_qvel[random_frame][:])\n",
        "\n",
        "    data = self.pipeline_init(qpos, qvel)\n",
        "\n",
        "    theta = jax.random.uniform(rng3, (), minval=0.0, maxval=2 * jp.pi)\n",
        "    d_star = jp.stack([jp.cos(theta), jp.sin(theta)])\n",
        "\n",
        "    state_info = {\n",
        "        'rng3': rng3,\n",
        "        'goal': d_star,\n",
        "        'v_xy': jp.zeros((2,), dtype=jp.float32),\n",
        "        'frame_count': jp.array(random_frame),\n",
        "        'mocap_qpos': qpos,\n",
        "        'mocap_qvel': qvel\n",
        "    }\n",
        "\n",
        "    obs = self._get_obs(data, jp.zeros(self.sys.nu), state_info)\n",
        "    reward, done, zero = jp.zeros(3)\n",
        "    metrics = {\n",
        "        'forward_reward': zero,\n",
        "        'reward_linvel': zero,\n",
        "        'reward_quadctrl': zero,\n",
        "        'reward_alive': zero,\n",
        "        'x_position': zero,\n",
        "        'y_position': zero,\n",
        "        'distance_from_origin': zero,\n",
        "        'x_velocity': zero,\n",
        "        'y_velocity': zero,\n",
        "    }\n",
        "\n",
        "    state = State(data, obs, reward, done, metrics, state_info)\n",
        "\n",
        "    # Sample a random 2D unit vector for the desired heading\n",
        "    # theta = jax.random.uniform(rng3, (), minval=0.0, maxval=2 * jp.pi)\n",
        "    # d_star = jp.stack([jp.cos(theta), jp.sin(theta)])\n",
        "    # state.info['goal'] = d_star\n",
        "    return state\n",
        "\n",
        "  def imit_reward(self, mo_pos, stt_pos, mo_vel, stt_vel):\n",
        "\n",
        "    mo_pos = jp.array(mo_pos)\n",
        "    stt_pos = jp.array(stt_pos)\n",
        "\n",
        "    pose_reward = 0\n",
        "    vel_reward = 0\n",
        "\n",
        "    rot_diff = quat_mul(mo_pos[3:7], quat_inv(stt_pos[3:7]))\n",
        "    diff_angle = quat_to_rotvec(rot_diff)\n",
        "    pose_reward = jp.sum(diff_angle**2)\n",
        "    #list of floats representing values in quaternion rotations\n",
        "    mo_rot_data = mo_pos[7:]\n",
        "    stt_rot_data = stt_pos[7:]\n",
        "\n",
        "    # print(\"mo_rot_data: \", len(mo_rot_data))\n",
        "    # print(\"stt_rot_data: \", len(stt_rot_data))\n",
        "\n",
        "    for r in range(0, len(mo_rot_data)):\n",
        "        rot_diff = mo_rot_data[r] - stt_rot_data[r]\n",
        "        pose_reward += (rot_diff ** 2)\n",
        "\n",
        "        #find difference between mocap rotation for given joint and the state rotation\n",
        "        #make negative so that when we raise e to it, higher difference leads to smaller reward\n",
        "        #pose_reward += (diff_quat) ** 2.0\n",
        "\n",
        "\n",
        "\n",
        "    #multiply by negative number so greater difference = more negative which means smaller fraction e is raised to it\n",
        "    #maxes out when there's no difference and reward ends up being 1\n",
        "    pose_reward *= -2.0\n",
        "\n",
        "    pose_reward = jp.exp(jp.maximum(pose_reward, -50))\n",
        "\n",
        "\n",
        "    mo_ang_vel = mo_vel[3:]\n",
        "    stt_ang_vel = stt_vel[3:]\n",
        "    # mo_ang_vel = mo_vel[6:]\n",
        "    # stt_ang_vel = stt_vel[6:]\n",
        "\n",
        "\n",
        "    for v in range(0, len(mo_ang_vel)):\n",
        "       vel_diff = mo_ang_vel[v] - stt_ang_vel[v]\n",
        "       vel_reward += (vel_diff ** 2.0)\n",
        "\n",
        "    vel_reward *= -0.1\n",
        "    vel_reward = jp.exp(jp.maximum(vel_reward, -50))\n",
        "\n",
        "\n",
        "    #first 3 values in both lists represent xyz pos of hips\n",
        "    center_mass_reward = jp.linalg.norm(mo_pos[:3] - stt_pos[:3])\n",
        "    center_mass_reward *= -10.0\n",
        "    center_mass_reward = jp.exp(jp.maximum(center_mass_reward, -50))\n",
        "\n",
        "    #full_reward = (0.7 * pose_reward) + (0.3 * center_mass_reward)\n",
        "    full_reward = (0.6 * pose_reward) + (0.3 * vel_reward)  + (0.1 * center_mass_reward)\n",
        "    return full_reward\n",
        "\n",
        "\n",
        "\n",
        "  def step(self, state: State, action: jp.ndarray) -> State:\n",
        "    current_frame = state.info['frame_count']\n",
        "    print(\"Current Frame:\", current_frame)\n",
        "    \"\"\"Runs one timestep of the environment's dynamics.\"\"\"\n",
        "    data0 = state.pipeline_state\n",
        "    data = self.pipeline_step(data0, action)\n",
        "\n",
        "    #FOR DEBUGGING MOCAP DATA\n",
        "    # data = data0\n",
        "    # hip_pos = data.qpos[:7]\n",
        "    # rest = self.frames_qpos[current_frame][7:]\n",
        "    # new_qpos = jp.concatenate([hip_pos, rest])\n",
        "    # data = self.pipeline_init(new_qpos, data.qvel)\n",
        "\n",
        "    com_before = data0.subtree_com[1]\n",
        "    com_after = data.subtree_com[1]\n",
        "    # velocity = (x_t - x_0) / t\n",
        "    velocity = (com_after - com_before) / self.dt\n",
        "\n",
        "    # # New Reward: Target Heading Task Reward\n",
        "    # d_star    = state.info['goal']\n",
        "    # v_xy   = velocity[:2] @ d_star # <--- v_xy = [v_x, v_y] @ [d_x, d_y]\n",
        "    # speed_err = jp.maximum(0.0, self._target_speed - v_xy)\n",
        "    # heading_r = jp.exp(-2.5 * speed_err**2)\n",
        "    # forward_reward = self._heading_weight * heading_r\n",
        "    # state.info['v_xy'] = velocity[:2]\n",
        "\n",
        "    hip_quat = data.x.rot[0]\n",
        "    forward_world = math.rotate(jp.array([1.0, 0.0, 0.0]), hip_quat) #rotate forward axis which is x to match up with quaternion rotation so this is characters forward\n",
        "    f_xy = forward_world[:2]\n",
        "    f_xy = f_xy/(jp.linalg.norm(f_xy) + 1e-6)#normalize\n",
        "\n",
        "    v_xy = velocity[0:2]/(jp.linalg.norm(velocity[0:2]) + 1e-6)\n",
        "    dir_rew = v_xy @ f_xy #forward direction dotted with velocity direction\n",
        "    forward_reward = self._forward_reward_weight * dir_rew\n",
        "    # Old Reward -- velocity[0] is x component of velocity {vx, vy, vz}\n",
        "    #forward_reward = self._forward_reward_weight * (velocity/jp.linalg.norm(velocity))[0] #forward_reward = 1.25 * velocity in the x direction\n",
        "\n",
        "    min_z, max_z = self._healthy_z_range\n",
        "    is_healthy = jp.where(data.q[2] < min_z, 0.0, 1.0)\n",
        "    is_healthy = jp.where(data.q[2] > max_z, 0.0, is_healthy)\n",
        "    is_healthy = jp.where(data.xpos[2][2] < (min_z + .3), 0.0, is_healthy)\n",
        "    is_healthy = jp.where(data.xpos[2][2] > (max_z + .3), 0.0, is_healthy)\n",
        "    if self._terminate_when_unhealthy:\n",
        "      healthy_reward = self._healthy_reward\n",
        "    else:\n",
        "      healthy_reward = self._healthy_reward * is_healthy\n",
        "\n",
        "    ctrl_cost = self._ctrl_cost_weight * jp.sum(jp.square(action))\n",
        "    frame_count = state.info['frame_count'] + 1\n",
        "    state_info = dict(state.info)\n",
        "    state_info['frame_count'] = frame_count\n",
        "\n",
        "    obs = self._get_obs(data, action, state.info)\n",
        "\n",
        "    #IMITATION REWARD!!!!!!\n",
        "    max_frame = self.frames_qpos.shape[0] - 1\n",
        "\n",
        "    mocap_qpos = self.frames_qpos[0]\n",
        "    mocap_qvel = self.frames_qvel[0]\n",
        "    mocap_qpos = jp.zeros_like(mocap_qpos)\n",
        "    mocap_qvel = jp.zeros_like(mocap_qvel)\n",
        "    def compute_reward(_):\n",
        "        mocap_qpos = self.frames_qpos[current_frame]\n",
        "        mocap_qvel = self.frames_qvel[current_frame]\n",
        "        return self.imit_reward(mocap_qpos, data.qpos, mocap_qvel, data.qvel)\n",
        "\n",
        "    def zero_reward(_):\n",
        "        return 0.0\n",
        "\n",
        "    imit_reward = lax.cond(\n",
        "        current_frame < max_frame,\n",
        "        compute_reward,\n",
        "        zero_reward,\n",
        "        operand=None\n",
        "    )\n",
        "\n",
        "    state_info['mocap_qpos'] = mocap_qpos\n",
        "    state_info['mocap_qvel'] = mocap_qvel\n",
        "\n",
        "    #GIVE IT SOME REWARD FOR NOT BENDING IT'S TORSO MUCH\n",
        "    torso_rot = data.qpos[7:10]\n",
        "    stiffness_reward = 0\n",
        "    for r in torso_rot:\n",
        "      stiffness_reward += r**2\n",
        "    stiffness_reward = jp.exp(stiffness_reward * -2)\n",
        "\n",
        "\n",
        "\n",
        "    #ADD IT ALL UP\n",
        "    reward = forward_reward + healthy_reward + stiffness_reward + (3.0 * imit_reward) - ctrl_cost\n",
        "\n",
        "    done = 1.0 - is_healthy if self._terminate_when_unhealthy else 0.0\n",
        "    state.metrics.update(\n",
        "        forward_reward=forward_reward,\n",
        "        reward_linvel=forward_reward,\n",
        "        reward_quadctrl=-ctrl_cost,\n",
        "        reward_alive=healthy_reward,\n",
        "        x_position=com_after[0],\n",
        "        y_position=com_after[1],\n",
        "        distance_from_origin=jp.linalg.norm(com_after),\n",
        "        x_velocity=velocity[0],\n",
        "        y_velocity=velocity[1],\n",
        "    )\n",
        "\n",
        "\n",
        "    return state.replace(\n",
        "        pipeline_state=data, obs=obs, reward=reward, done=done, info=state_info\n",
        "    )\n",
        "\n",
        "  def _get_obs(\n",
        "      self, data: mjx.Data, action: jp.ndarray, state_info: dict[str, Any],\n",
        "  ) -> jp.ndarray:\n",
        "    \"\"\"Observes humanoid body position, velocities, and angles.\"\"\"\n",
        "\n",
        "    position = data.qpos\n",
        "    if self._exclude_current_positions_from_observation:\n",
        "      position = position[2:]\n",
        "\n",
        "    # print(\"Print data.qpos in _get_obs:\", data.qpos)\n",
        "    # print(\"Print data.qvel in _get_obs:\", data.qvel)\n",
        "    # # mass and inertia tensor in the center of mass (COM) frame.\n",
        "    # print(\"Print flattened data.cinert in _get_obs:\", data.cinert.ravel())\n",
        "    # print(\"Print flattened data.cvel in _get_obs:\", data.cvel.ravel())\n",
        "    # print(\"Print data.qfrc_actuator in _get_obs:\", data.qfrc_actuator)\n",
        "\n",
        "    # external_contact_forces are excluded\n",
        "    return jp.concatenate([\n",
        "        position,\n",
        "        data.qvel,\n",
        "        data.cinert[1:].ravel(),\n",
        "        data.cvel[1:].ravel(),\n",
        "        data.qfrc_actuator,\n",
        "        state_info['mocap_qpos'],\n",
        "        state_info['mocap_qvel']\n",
        "        # state_info['goal'],\n",
        "        # state_info['v_xy'],\n",
        "    ])\n",
        "\n",
        "\n",
        "# register env class we just made\n",
        "envs.register_environment('humanoid', Humanoid)\n",
        "\n",
        "# set the env as this humanoid env\n",
        "env_name = 'humanoid'\n",
        "env = envs.get_environment(env_name)\n",
        "\n",
        "# define the jit reset/step functions to put it on the GPU\n",
        "jit_reset = jax.jit(env.reset)\n",
        "jit_step = jax.jit(env.step)"
      ],
      "metadata": {
        "id": "os4N8vNR3mf5",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Define Humanoid Env for Martial Arts\n",
        "HUMANOID_ROOT_PATH = epath.Path(epath.resource_path('mujoco')) / 'mjx/test_data/humanoid'\n",
        "FIXED_ROOT_PATH = epath.Path('mujoco_resources/humanoid_CMU_folder')\n",
        "from jax import lax\n",
        "from jax import random\n",
        "\n",
        "\n",
        "def quat_inv(q):\n",
        "    \"\"\"Returns the inverse (conjugate) of a unit quaternion [w, x, y, z].\"\"\"\n",
        "    w, x, y, z = q\n",
        "    return jp.array([w, -x, -y, -z])\n",
        "\n",
        "def quat_mul(q1, q2):\n",
        "        \"\"\"Returns the multiplication of two quaternions [w, x, y, z].\"\"\"\n",
        "        w1, x1, y1, z1 = q1\n",
        "        w2, x2, y2, z2 = q2\n",
        "        return jp.array([\n",
        "            w1*w2 - x1*x2 - y1*y2 - z1*z2,\n",
        "            w1*x2 + x1*w2 + y1*z2 - z1*y2,\n",
        "            w1*y2 - x1*z2 + y1*w2 + z1*x2,\n",
        "            w1*z2 + x1*y2 - y1*x2 + z1*w2\n",
        "        ])\n",
        "\n",
        "def quat_to_rotvec(q):\n",
        "        \"\"\"Convert a unit quaternion [w, x, y, z] to axis-angle vector (rotvec).\"\"\"\n",
        "        norm_q = q / jp.linalg.norm(q)\n",
        "        w, xyz = norm_q[0], norm_q[1:]\n",
        "        sin_half_theta = jp.linalg.norm(xyz)\n",
        "        angle = 2.0 * jp.arctan2(sin_half_theta, w)\n",
        "        axis = jp.where(sin_half_theta > 1e-6, xyz / sin_half_theta, jp.zeros_like(xyz))\n",
        "        return angle * axis\n",
        "\n",
        "class Humanoid(PipelineEnv):\n",
        "  def __init__(\n",
        "      self,\n",
        "      forward_reward_weight=1.25,\n",
        "      ctrl_cost_weight=0.1,\n",
        "      healthy_reward=2.0,\n",
        "      terminate_when_unhealthy=True,\n",
        "      healthy_z_range=(0.5, 10.0),\n",
        "      reset_noise_scale=1e-2,\n",
        "      exclude_current_positions_from_observation=True,\n",
        "      target_speed = 1.0,\n",
        "      heading_reward_weight= 5.0,\n",
        "      **kwargs,\n",
        "  ):\n",
        "\n",
        "    # Option 1: TRAIN WITHOUT HEIGHT FIELD-- plain humanoid\n",
        "    # mj_model = mujoco.MjModel.from_xml_path(\n",
        "    #     (HUMANOID_ROOT_PATH / 'humanoid.xml').as_posix())\n",
        "\n",
        "    # Option 2: TRAIN WITH HEIGH FIELD-- humanoid with terrain\n",
        "    # mj_model = mujoco.MjModel.from_xml_path(\n",
        "    #     (TERRAIN_ROOT_PATH / 'humanoid.xml').as_posix())\n",
        "    mj_model = mujoco.MjModel.from_xml_path(\n",
        "         (FIXED_ROOT_PATH / 'humanoid_fixed_7.2.xml').as_posix())\n",
        "\n",
        "    mj_model.opt.solver = mujoco.mjtSolver.mjSOL_CG\n",
        "    mj_model.opt.iterations = 6\n",
        "    mj_model.opt.ls_iterations = 6\n",
        "\n",
        "    sys = mjcf.load_model(mj_model)\n",
        "\n",
        "    physics_steps_per_control_step = 5\n",
        "\n",
        "    kwargs['n_frames'] = kwargs.get(\n",
        "        'n_frames', physics_steps_per_control_step)\n",
        "    kwargs['backend'] = 'mjx'\n",
        "\n",
        "    super().__init__(sys, **kwargs)\n",
        "\n",
        "    # Original Rewards/Costs\n",
        "    self._forward_reward_weight = forward_reward_weight\n",
        "    self._ctrl_cost_weight = ctrl_cost_weight\n",
        "    self._healthy_reward = healthy_reward\n",
        "    self._terminate_when_unhealthy = terminate_when_unhealthy\n",
        "    self._healthy_z_range = healthy_z_range\n",
        "    self._reset_noise_scale = reset_noise_scale\n",
        "    self._exclude_current_positions_from_observation = (\n",
        "        exclude_current_positions_from_observation\n",
        "    )\n",
        "\n",
        "    # New Parameters\n",
        "    self._target_speed = target_speed\n",
        "    self._heading_weight= heading_reward_weight\n",
        "\n",
        "    #mocap stuff\n",
        "    self.mocap_dict = parse_bvh_file()\n",
        "    self.frames = self.mocap_dict[\"martial_arts\"][0]\n",
        "    self.frames_qvel = []\n",
        "    self.frames_qpos = []\n",
        "    for frame in self.frames:\n",
        "      qpos = frame[\"qpos\"]\n",
        "      qvel = frame[\"qvel\"]\n",
        "      self.frames_qpos.append(qpos)\n",
        "      self.frames_qvel.append(qvel)\n",
        "    #each row is a frame\n",
        "    self.frames_qpos = np.array(self.frames_qpos)\n",
        "    self.frames_qvel = np.array(self.frames_qvel)\n",
        "    print(self.frames_qpos. shape[0], \", \", self.frames_qpos.shape[1])\n",
        "    self.frames_qpos = jp.array(self.frames_qpos)\n",
        "    self.frames_qvel = jp.array(self.frames_qvel)\n",
        "    self.step_count = 0\n",
        "    self.key = random.PRNGKey(42)  # Set the seed ONCE\n",
        "    self.key, subkey = random.split(self.key)\n",
        "    self.random_frame = random.randint(subkey, shape=(), minval=0, maxval=2000)\n",
        "    print(\"rand frame: \", self.random_frame)\n",
        "\n",
        "  def reset(self, rng: jp.ndarray) -> State:\n",
        "    \"\"\"Resets the environment to an initial state.\"\"\"\n",
        "    # key = random.PRNGKey(rng3)  # Set the seed\n",
        "    # key, subkey = random.split(self.key)\n",
        "    rng, rng1, rng2, rng3 = jax.random.split(rng, 4)\n",
        "    random_frame = random.randint(rng1, shape=(), minval=0, maxval=2000)\n",
        "    #jax.debug.print(\"Random frame: {}\", random_frame)\n",
        "\n",
        "    low, hi = -self._reset_noise_scale, self._reset_noise_scale\n",
        "    qpos = self.sys.qpos0 + jax.random.uniform(\n",
        "        rng1, (self.sys.nq,), minval=low, maxval=hi\n",
        "    )\n",
        "    qvel = jax.random.uniform(\n",
        "        rng2, (self.sys.nv,), minval=low, maxval=hi\n",
        "    )\n",
        "\n",
        "    #RANDOMLY INITIALIZE TO SOME RANDOM FRAME IN THE MOCAP\n",
        "    qpos = qpos.at[:].set(self.frames_qpos[random_frame][:])\n",
        "    qvel = qvel.at[:].set(self.frames_qvel[random_frame][:])\n",
        "\n",
        "    data = self.pipeline_init(qpos, qvel)\n",
        "\n",
        "    theta = jax.random.uniform(rng3, (), minval=0.0, maxval=2 * jp.pi)\n",
        "    d_star = jp.stack([jp.cos(theta), jp.sin(theta)])\n",
        "\n",
        "    state_info = {\n",
        "        'rng3': rng3,\n",
        "        'goal': d_star,\n",
        "        'v_xy': jp.zeros((2,), dtype=jp.float32),\n",
        "        'frame_count': jp.array(random_frame),\n",
        "        'mocap_qpos': qpos,\n",
        "        'mocap_qvel': qvel\n",
        "    }\n",
        "\n",
        "    obs = self._get_obs(data, jp.zeros(self.sys.nu), state_info)\n",
        "    reward, done, zero = jp.zeros(3)\n",
        "    metrics = {\n",
        "        'forward_reward': zero,\n",
        "        'reward_linvel': zero,\n",
        "        'reward_quadctrl': zero,\n",
        "        'reward_alive': zero,\n",
        "        'x_position': zero,\n",
        "        'y_position': zero,\n",
        "        'distance_from_origin': zero,\n",
        "        'x_velocity': zero,\n",
        "        'y_velocity': zero,\n",
        "    }\n",
        "\n",
        "    state = State(data, obs, reward, done, metrics, state_info)\n",
        "\n",
        "    # Sample a random 2D unit vector for the desired heading\n",
        "    # theta = jax.random.uniform(rng3, (), minval=0.0, maxval=2 * jp.pi)\n",
        "    # d_star = jp.stack([jp.cos(theta), jp.sin(theta)])\n",
        "    # state.info['goal'] = d_star\n",
        "    return state\n",
        "\n",
        "  def imit_reward(self, mo_pos, stt_pos, mo_vel, stt_vel):\n",
        "\n",
        "    mo_pos = jp.array(mo_pos)\n",
        "    stt_pos = jp.array(stt_pos)\n",
        "\n",
        "    pose_reward = 0\n",
        "    vel_reward = 0\n",
        "\n",
        "    rot_diff = quat_mul(mo_pos[3:7], quat_inv(stt_pos[3:7]))\n",
        "    diff_angle = quat_to_rotvec(rot_diff)\n",
        "    pose_reward = jp.sum(diff_angle**2)\n",
        "    #list of floats representing values in quaternion rotations\n",
        "    mo_rot_data = mo_pos[7:]\n",
        "    stt_rot_data = stt_pos[7:]\n",
        "\n",
        "    # print(\"mo_rot_data: \", len(mo_rot_data))\n",
        "    # print(\"stt_rot_data: \", len(stt_rot_data))\n",
        "\n",
        "    for r in range(0, len(mo_rot_data)):\n",
        "        rot_diff = mo_rot_data[r] - stt_rot_data[r]\n",
        "        pose_reward += (rot_diff ** 2)\n",
        "\n",
        "        #find difference between mocap rotation for given joint and the state rotation\n",
        "        #make negative so that when we raise e to it, higher difference leads to smaller reward\n",
        "        #pose_reward += (diff_quat) ** 2.0\n",
        "\n",
        "\n",
        "\n",
        "    #multiply by negative number so greater difference = more negative which means smaller fraction e is raised to it\n",
        "    #maxes out when there's no difference and reward ends up being 1\n",
        "    pose_reward *= -2.0\n",
        "\n",
        "    pose_reward = jp.exp(jp.maximum(pose_reward, -50))\n",
        "\n",
        "\n",
        "    mo_ang_vel = mo_vel[3:]\n",
        "    stt_ang_vel = stt_vel[3:]\n",
        "    # mo_ang_vel = mo_vel[6:]\n",
        "    # stt_ang_vel = stt_vel[6:]\n",
        "\n",
        "\n",
        "    for v in range(0, len(mo_ang_vel)):\n",
        "       vel_diff = mo_ang_vel[v] - stt_ang_vel[v]\n",
        "       vel_reward += (vel_diff ** 2.0)\n",
        "\n",
        "    vel_reward *= -0.1\n",
        "    vel_reward = jp.exp(jp.maximum(vel_reward, -50))\n",
        "\n",
        "\n",
        "    #first 3 values in both lists represent xyz pos of hips\n",
        "    center_mass_reward = jp.linalg.norm(mo_pos[:3] - stt_pos[:3])\n",
        "    center_mass_reward *= -10.0\n",
        "    center_mass_reward = jp.exp(jp.maximum(center_mass_reward, -50))\n",
        "\n",
        "    #full_reward = (0.7 * pose_reward) + (0.3 * center_mass_reward)\n",
        "    full_reward = (0.6 * pose_reward) + (0.3 * vel_reward)  + (0.1 * center_mass_reward)\n",
        "    return full_reward\n",
        "\n",
        "\n",
        "\n",
        "  def step(self, state: State, action: jp.ndarray) -> State:\n",
        "    current_frame = state.info['frame_count']\n",
        "    print(\"Current Frame:\", current_frame)\n",
        "    \"\"\"Runs one timestep of the environment's dynamics.\"\"\"\n",
        "    data0 = state.pipeline_state\n",
        "    data = self.pipeline_step(data0, action)\n",
        "\n",
        "    #FOR DEBUGGING MOCAP DATA\n",
        "    # data = data0\n",
        "    # hip_pos = data.qpos[:7]\n",
        "    # rest = self.frames_qpos[current_frame][7:]\n",
        "    # new_qpos = jp.concatenate([hip_pos, rest])\n",
        "    # data = self.pipeline_init(new_qpos, data.qvel)\n",
        "\n",
        "    com_before = data0.subtree_com[1]\n",
        "    com_after = data.subtree_com[1]\n",
        "    # velocity = (x_t - x_0) / t\n",
        "    velocity = (com_after - com_before) / self.dt\n",
        "\n",
        "    # # New Reward: Target Heading Task Reward\n",
        "    # d_star    = state.info['goal']\n",
        "    # v_xy   = velocity[:2] @ d_star # <--- v_xy = [v_x, v_y] @ [d_x, d_y]\n",
        "    # speed_err = jp.maximum(0.0, self._target_speed - v_xy)\n",
        "    # heading_r = jp.exp(-2.5 * speed_err**2)\n",
        "    # forward_reward = self._heading_weight * heading_r\n",
        "    # state.info['v_xy'] = velocity[:2]\n",
        "\n",
        "\n",
        "    ######################################### walking in circle forward reward\n",
        "    # hip_quat = data.x.rot[0]\n",
        "    # forward_world = math.rotate(jp.array([1.0, 0.0, 0.0]), hip_quat) #rotate forward axis which is x to match up with quaternion rotation so this is characters forward\n",
        "    # f_xy = forward_world[:2]\n",
        "    # f_xy = f_xy/(jp.linalg.norm(f_xy) + 1e-6)#normalize\n",
        "\n",
        "    # v_xy = velocity[0:2]/(jp.linalg.norm(velocity[0:2]) + 1e-6)\n",
        "    # dir_rew = v_xy @ f_xy #forward direction dotted with velocity direction\n",
        "    # forward_reward = self._forward_reward_weight * dir_rew\n",
        "    #########################################\n",
        "\n",
        "    # Zero Reward -- for martial arts one\n",
        "    forward_reward = 0.0\n",
        "\n",
        "    # Old Reward -- velocity[0] is x component of velocity {vx, vy, vz}\n",
        "    #forward_reward = self._forward_reward_weight * (velocity/jp.linalg.norm(velocity))[0] #forward_reward = 1.25 * velocity in the x direction\n",
        "\n",
        "    min_z, max_z = self._healthy_z_range\n",
        "    is_healthy = jp.where(data.q[2] < min_z, 0.0, 1.0)\n",
        "    is_healthy = jp.where(data.q[2] > max_z, 0.0, is_healthy)\n",
        "    is_healthy = jp.where(data.xpos[2][2] < (min_z + .3), 0.0, is_healthy)\n",
        "    is_healthy = jp.where(data.xpos[2][2] > (max_z + .3), 0.0, is_healthy)\n",
        "    if self._terminate_when_unhealthy:\n",
        "      healthy_reward = self._healthy_reward\n",
        "    else:\n",
        "      healthy_reward = self._healthy_reward * is_healthy\n",
        "\n",
        "    ctrl_cost = self._ctrl_cost_weight * jp.sum(jp.square(action))\n",
        "    frame_count = state.info['frame_count'] + 1\n",
        "    state_info = dict(state.info)\n",
        "    state_info['frame_count'] = frame_count\n",
        "\n",
        "    obs = self._get_obs(data, action, state.info)\n",
        "\n",
        "    #IMITATION REWARD!!!!!!\n",
        "    max_frame = self.frames_qpos.shape[0] - 1\n",
        "\n",
        "    mocap_qpos = self.frames_qpos[0]\n",
        "    mocap_qvel = self.frames_qvel[0]\n",
        "    mocap_qpos = jp.zeros_like(mocap_qpos)\n",
        "    mocap_qvel = jp.zeros_like(mocap_qvel)\n",
        "    def compute_reward(_):\n",
        "        mocap_qpos = self.frames_qpos[current_frame]\n",
        "        mocap_qvel = self.frames_qvel[current_frame]\n",
        "        return self.imit_reward(mocap_qpos, data.qpos, mocap_qvel, data.qvel)\n",
        "\n",
        "    def zero_reward(_):\n",
        "        return 0.0\n",
        "\n",
        "    imit_reward = lax.cond(\n",
        "        current_frame < max_frame,\n",
        "        compute_reward,\n",
        "        zero_reward,\n",
        "        operand=None\n",
        "    )\n",
        "\n",
        "    state_info['mocap_qpos'] = mocap_qpos\n",
        "    state_info['mocap_qvel'] = mocap_qvel\n",
        "\n",
        "    #GIVE IT SOME REWARD FOR NOT BENDING IT'S TORSO MUCH\n",
        "    torso_rot = data.qpos[7:10]\n",
        "    stiffness_reward = 0\n",
        "    for r in torso_rot:\n",
        "      stiffness_reward += r**2\n",
        "    stiffness_reward = jp.exp(stiffness_reward * -2)\n",
        "\n",
        "\n",
        "    #ADD IT ALL UP\n",
        "    #reward = forward_reward + healthy_reward + stiffness_reward + (3.0 * imit_reward) - ctrl_cost\n",
        "\n",
        "    # ADD IT ALL UP (FOR MARTIAL ARTS) ---\n",
        "    reward = healthy_reward + stiffness_reward + (6.0 * imit_reward) - ctrl_cost\n",
        "\n",
        "\n",
        "    done = 1.0 - is_healthy if self._terminate_when_unhealthy else 0.0\n",
        "    state.metrics.update(\n",
        "        forward_reward=forward_reward,\n",
        "        reward_linvel=forward_reward,\n",
        "        reward_quadctrl=-ctrl_cost,\n",
        "        reward_alive=healthy_reward,\n",
        "        x_position=com_after[0],\n",
        "        y_position=com_after[1],\n",
        "        distance_from_origin=jp.linalg.norm(com_after),\n",
        "        x_velocity=velocity[0],\n",
        "        y_velocity=velocity[1],\n",
        "    )\n",
        "\n",
        "\n",
        "    return state.replace(\n",
        "        pipeline_state=data, obs=obs, reward=reward, done=done, info=state_info\n",
        "    )\n",
        "\n",
        "  def _get_obs(\n",
        "      self, data: mjx.Data, action: jp.ndarray, state_info: dict[str, Any],\n",
        "  ) -> jp.ndarray:\n",
        "    \"\"\"Observes humanoid body position, velocities, and angles.\"\"\"\n",
        "\n",
        "    position = data.qpos\n",
        "    if self._exclude_current_positions_from_observation:\n",
        "      position = position[2:]\n",
        "\n",
        "    # print(\"Print data.qpos in _get_obs:\", data.qpos)\n",
        "    # print(\"Print data.qvel in _get_obs:\", data.qvel)\n",
        "    # # mass and inertia tensor in the center of mass (COM) frame.\n",
        "    # print(\"Print flattened data.cinert in _get_obs:\", data.cinert.ravel())\n",
        "    # print(\"Print flattened data.cvel in _get_obs:\", data.cvel.ravel())\n",
        "    # print(\"Print data.qfrc_actuator in _get_obs:\", data.qfrc_actuator)\n",
        "\n",
        "    # external_contact_forces are excluded\n",
        "    return jp.concatenate([\n",
        "        position,\n",
        "        data.qvel,\n",
        "        data.cinert[1:].ravel(),\n",
        "        data.cvel[1:].ravel(),\n",
        "        data.qfrc_actuator,\n",
        "        state_info['mocap_qpos'],\n",
        "        state_info['mocap_qvel']\n",
        "        # state_info['goal'],\n",
        "        # state_info['v_xy'],\n",
        "    ])\n",
        "\n",
        "\n",
        "# register env class we just made\n",
        "envs.register_environment('humanoid', Humanoid)\n",
        "\n",
        "# set the env as this humanoid env\n",
        "env_name = 'humanoid'\n",
        "env = envs.get_environment(env_name)\n",
        "\n",
        "# define the jit reset/step functions to put it on the GPU\n",
        "jit_reset = jax.jit(env.reset)\n",
        "jit_step = jax.jit(env.step)"
      ],
      "metadata": {
        "id": "HDm1H1R_oStK",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train\n"
      ],
      "metadata": {
        "id": "-dzwf-MCY46R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Train\n",
        "train_fn = functools.partial(\n",
        "    ppo.train, num_timesteps=5_000_000, num_evals=5, reward_scaling=0.1,\n",
        "    episode_length=1000, normalize_observations=True, action_repeat=1,\n",
        "    unroll_length=10, num_minibatches=24, num_updates_per_batch=8,\n",
        "    discounting=0.97, learning_rate=3e-4, entropy_cost=1e-3, num_envs=3072,\n",
        "    batch_size=512, seed=0)\n",
        "\n",
        "x_data = []\n",
        "y_data = []\n",
        "ydataerr = []\n",
        "times = [datetime.now()]\n",
        "\n",
        "max_y, min_y = 13000, 0\n",
        "\n",
        "def progress(num_steps, metrics):\n",
        "  times.append(datetime.now())\n",
        "  x_data.append(num_steps)\n",
        "  y_data.append(metrics['eval/episode_reward'])\n",
        "  ydataerr.append(metrics['eval/episode_reward_std'])\n",
        "\n",
        "\n",
        "  reward = metrics['eval/episode_reward']\n",
        "  reward_std = metrics['eval/episode_reward_std']\n",
        "\n",
        "  print(f\"Step: {num_steps} | Eval Reward: {reward:.2f} ± {reward_std:.2f}\")\n",
        "\n",
        "make_inference_fn, params, _= train_fn(environment=env, progress_fn=progress)\n",
        "\n",
        "print(f'time to jit: {times[1] - times[0]}')\n",
        "print(f'time to train: {times[-1] - times[1]}')"
      ],
      "metadata": {
        "id": "FvhBtFMJG6L4",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Train whilst Saving Checkpoints\n",
        "ckpt_path = epath.Path('/tmp/humanoid_imitating_hips_120_martial_arts_7.2/ckpts')\n",
        "ckpt_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "def save_policy_params_fn(current_step, make_policy, params):\n",
        "  # save checkpoints\n",
        "  orbax_checkpointer = ocp.PyTreeCheckpointer()\n",
        "  save_args = orbax_utils.save_args_from_target(params)\n",
        "  path = ckpt_path / f'{current_step}'\n",
        "  orbax_checkpointer.save(path, params, force=True, save_args=save_args)\n",
        "\n",
        "train_fn = functools.partial(\n",
        "    ppo.train, num_timesteps=10_000_000, num_evals=20, reward_scaling=0.1,\n",
        "    episode_length=1000, normalize_observations=True, action_repeat=1,\n",
        "    unroll_length=10, num_minibatches=24, num_updates_per_batch=8,\n",
        "    discounting=0.97, learning_rate=3e-4, entropy_cost=1e-3, num_envs=3072,\n",
        "    batch_size=512, policy_params_fn=save_policy_params_fn, seed=0)\n",
        "\n",
        "x_data = []\n",
        "y_data = []\n",
        "ydataerr = []\n",
        "times = [datetime.now()]\n",
        "\n",
        "max_y, min_y = 13000, 0\n",
        "\n",
        "def progress(num_steps, metrics):\n",
        "  times.append(datetime.now())\n",
        "  x_data.append(num_steps)\n",
        "  y_data.append(metrics['eval/episode_reward'])\n",
        "  ydataerr.append(metrics['eval/episode_reward_std'])\n",
        "  reward = metrics['eval/episode_reward']\n",
        "  reward_std = metrics['eval/episode_reward_std']\n",
        "\n",
        "  print(f\"Step: {num_steps} | Eval Reward: {reward:.2f} ± {reward_std:.2f}\")\n",
        "\n",
        "make_inference_fn, params, _= train_fn(environment=env, progress_fn=progress)\n",
        "\n",
        "print(f'time to jit: {times[1] - times[0]}')\n",
        "print(f'time to train: {times[-1] - times[1]}')"
      ],
      "metadata": {
        "id": "ZTzGMoxAvnt1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Save Checkpoints to Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "!cp -r /tmp/humanoid_imitating_hips_120_martial_arts_7.2/ckpts /content/drive/MyDrive/humanoid_72_martial_arts"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "79c50DhB-pg0",
        "outputId": "98b376f5-f416-4677-ad07-3af8cf328257"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Train from the Last Checkpoint\n",
        "ckpt_path = epath.Path('/content/drive/MyDrive/humanoid_72_martial_arts')\n",
        "ckpt_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "latest_ckpts = list(ckpt_path.glob('*'))\n",
        "latest_ckpts.sort(key=lambda x: int(x.as_posix().split('/')[-1]))\n",
        "latest_ckpt = latest_ckpts[-1]\n",
        "\n",
        "# If last ckpt is in google drive\n",
        "# ckpt_path = pathlib.Path('/content/drive/MyDrive/humanoid_training/ckpts')\n",
        "# latest_ckpt = sorted(ckpt_path.iterdir(), key=lambda p: int(p.name))[-1]\n",
        "\n",
        "save_path_for_training_from_checkpoint = epath.Path('/tmp/humanoid_imitating_hips_72_40_M/ckpts')\n",
        "save_path_for_training_from_checkpoint.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "def save_policy_params_fn(current_step, make_policy, params):\n",
        "  # save checkpoints\n",
        "  orbax_checkpointer = ocp.PyTreeCheckpointer()\n",
        "  save_args = orbax_utils.save_args_from_target(params)\n",
        "  path = save_path_for_training_from_checkpoint / f'{current_step}'\n",
        "  orbax_checkpointer.save(path, params, force=True, save_args=save_args)\n",
        "\n",
        "\n",
        "train_fn = functools.partial(\n",
        "    ppo.train, num_timesteps=30_000_000, num_evals=20, reward_scaling=0.1,\n",
        "    episode_length=1000, normalize_observations=True, action_repeat=1,\n",
        "    unroll_length=10, num_minibatches=24, num_updates_per_batch=8,\n",
        "    discounting=0.97, learning_rate=3e-4, entropy_cost=1e-3, num_envs=3072,\n",
        "    batch_size=512, policy_params_fn=save_policy_params_fn, seed=0, restore_checkpoint_path=latest_ckpt)\n",
        "\n",
        "x_data = []\n",
        "y_data = []\n",
        "ydataerr = []\n",
        "times = [datetime.now()]\n",
        "\n",
        "max_y, min_y = 13000, 0\n",
        "\n",
        "def progress(num_steps, metrics):\n",
        "  times.append(datetime.now())\n",
        "  x_data.append(num_steps)\n",
        "  y_data.append(metrics['eval/episode_reward'])\n",
        "  ydataerr.append(metrics['eval/episode_reward_std'])\n",
        "\n",
        "  reward = metrics['eval/episode_reward']\n",
        "  reward_std = metrics['eval/episode_reward_std']\n",
        "\n",
        "  print(f\"Step: {num_steps} | Eval Reward: {reward:.2f} ± {reward_std:.2f}\")\n",
        "\n",
        "make_inference_fn, params, _= train_fn(environment=env, progress_fn=progress)\n",
        "\n",
        "print(\"Done.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bf3a6RTHL2T5",
        "outputId": "4b2473d7-b468-47b5-d699-ed22b6aa25b9",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current Frame: Traced<ShapedArray(int32[])>with<BatchTrace> with\n",
            "  val = Traced<ShapedArray(int32[128])>with<DynamicJaxprTrace>\n",
            "  batch_dim = 0\n",
            "Step: 0 | Eval Reward: 522.82 ± 129.33\n",
            "Current Frame: Traced<ShapedArray(int32[])>with<BatchTrace> with\n",
            "  val = Traced<ShapedArray(int32[3072])>with<DynamicJaxprTrace>\n",
            "  batch_dim = 0\n",
            "Step: 1597440 | Eval Reward: 538.78 ± 163.26\n",
            "Step: 3194880 | Eval Reward: 571.99 ± 158.18\n",
            "Step: 4792320 | Eval Reward: 582.93 ± 221.38\n",
            "Step: 6389760 | Eval Reward: 605.13 ± 194.41\n",
            "Step: 7987200 | Eval Reward: 638.58 ± 193.96\n",
            "Step: 9584640 | Eval Reward: 624.46 ± 147.03\n",
            "Step: 11182080 | Eval Reward: 608.33 ± 193.64\n",
            "Step: 12779520 | Eval Reward: 629.80 ± 159.70\n",
            "Step: 14376960 | Eval Reward: 648.22 ± 207.79\n",
            "Step: 15974400 | Eval Reward: 643.68 ± 197.86\n",
            "Step: 17571840 | Eval Reward: 645.43 ± 201.60\n",
            "Step: 19169280 | Eval Reward: 681.43 ± 252.31\n",
            "Step: 20766720 | Eval Reward: 662.79 ± 189.63\n",
            "Step: 22364160 | Eval Reward: 671.14 ± 240.23\n",
            "Step: 23961600 | Eval Reward: 544.75 ± 179.14\n",
            "Step: 25559040 | Eval Reward: 610.93 ± 196.29\n",
            "Step: 27156480 | Eval Reward: 670.31 ± 255.42\n",
            "Step: 28753920 | Eval Reward: 664.42 ± 207.02\n",
            "Step: 30351360 | Eval Reward: 654.55 ± 198.22\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Save Checkpoints to Drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "!cp -r /tmp/humanoid_imitating_hips_72_40_M/ckpts /content/drive/MyDrive/hi_ya_40_000_000"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z2O1uWmGL7Fk",
        "outputId": "f4c3768e-438a-43ae-a9cc-232550597548",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Save Trained Model\n",
        "model_path = '/tmp/humanoid_fixed_7_hi_ya_40_M'\n",
        "model.save_params(model_path, params)\n",
        "from google.colab import files\n",
        "files.download(model_path)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "VSikS4SWZZJU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Render"
      ],
      "metadata": {
        "id": "HNJITpRTZTCg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Visualize Currently Activated Env\n",
        "state = jit_reset(jax.random.PRNGKey(0))\n",
        "rollout = [state.pipeline_state]\n",
        "\n",
        "# grab a trajectory\n",
        "for i in range(2300):\n",
        "  ctrl = -0.1 * jp.ones(env.sys.nu)\n",
        "  state = jit_step(state, ctrl)\n",
        "  rollout.append(state.pipeline_state)\n",
        "  print(\"qpos:\", state.pipeline_state.qpos)\n",
        "  #print(\"qpos_length:\", len(state.pipeline_state.qpos))\n",
        "  #print(\"qvel:\", state.pipeline_state.qvel)\n",
        "  #print(\"qvel_length:\", len(state.pipeline_state.qvel))\n",
        "\n",
        "media.show_video(env.render(rollout, camera='side'), fps=1.0 / env.dt)"
      ],
      "metadata": {
        "id": "JHZkoXYeQ6k_",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Render Trained Policy\n",
        "# NOTE: Make sure to play/activate the current Humanoid Env you use to train the policy you wanted to render, so there is not shape mismatches.\n",
        "train_fn = functools.partial(\n",
        "    ppo.train, num_timesteps=0, num_evals=0, reward_scaling=0.1,\n",
        "    episode_length=1000, normalize_observations=True, action_repeat=1,\n",
        "    unroll_length=10, num_minibatches=24, num_updates_per_batch=8,\n",
        "    discounting=0.97, learning_rate=3e-4, entropy_cost=1e-3, num_envs=3072,\n",
        "    batch_size=512, seed=0)\n",
        "\n",
        "x_data = []\n",
        "y_data = []\n",
        "ydataerr = []\n",
        "times = [datetime.now()]\n",
        "max_y, min_y = 13000, 0\n",
        "\n",
        "def progress(num_steps, metrics):\n",
        "  times.append(datetime.now())\n",
        "  x_data.append(num_steps)\n",
        "  y_data.append(metrics['eval/episode_reward'])\n",
        "  ydataerr.append(metrics['eval/episode_reward_std'])\n",
        "  reward = metrics['eval/episode_reward']\n",
        "  reward_std = metrics['eval/episode_reward_std']\n",
        "  print(f\"Step: {num_steps} | Eval Reward: {reward:.2f} ± {reward_std:.2f}\")\n",
        "make_inference_fn, params, _= train_fn(environment=env, progress_fn=progress)\n",
        "\n",
        "print('Load Model...')\n",
        "model_path = '/tmp/humanoid_fixed_7_hi_ya_40_M'\n",
        "# model_path = '/content/drive/MyDrive/with_hip_data_imitation_after_good_walk_longer'\n",
        "# model_path = '/content/drive/MyDrive/run_policy'\n",
        "\n",
        "params = model.load_params(model_path)\n",
        "\n",
        "inference_fn = make_inference_fn(params) # <----- plugs the weights in\n",
        "\n",
        "jit_inference_fn = jax.jit(inference_fn)\n",
        "\n",
        "eval_env = envs.get_environment('humanoid')\n",
        "\n",
        "jit_reset = jax.jit(eval_env.reset)\n",
        "jit_step = jax.jit(eval_env.step)\n",
        "rng = jax.random.PRNGKey(4)\n",
        "state = jit_reset(rng)\n",
        "\n",
        "# theta = jp.deg2rad(0)\n",
        "# d_star = jp.stack([jp.cos(theta), jp.sin(theta)])\n",
        "# state.info['goal'] = d_star\n",
        "\n",
        "rollout = [state.pipeline_state] # <---- initliazie rollout list with the first reset state\n",
        "\n",
        "n_steps = 5000\n",
        "render_every = 2\n",
        "\n",
        "for i in range(n_steps):\n",
        "  act_rng, rng = jax.random.split(rng)\n",
        "  ctrl, _ = jit_inference_fn(state.obs, act_rng) # this is where your model is making the inference!\n",
        "  state = jit_step(state, ctrl)\n",
        "  rollout.append(state.pipeline_state)\n",
        "\n",
        "  if state.done:\n",
        "    break\n",
        "\n",
        "media.show_video(env.render(rollout[::render_every], camera='side', height=1080, width=1920), fps=1.0 / env.dt / render_every)\n",
        "\n",
        "print(\"Done.\")"
      ],
      "metadata": {
        "id": "X5zp9cB7A2Dc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}